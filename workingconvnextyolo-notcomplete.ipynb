{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11741902,"sourceType":"datasetVersion","datasetId":7370967}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install ultralytics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T00:37:41.901994Z","iopub.execute_input":"2025-09-10T00:37:41.902216Z","iopub.status.idle":"2025-09-10T00:38:54.075179Z","shell.execute_reply.started":"2025-09-10T00:37:41.902192Z","shell.execute_reply":"2025-09-10T00:38:54.074199Z"}},"outputs":[{"name":"stdout","text":"Collecting ultralytics\n  Downloading ultralytics-8.3.197-py3-none-any.whl.metadata (37 kB)\nRequirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.7.2)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.4)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (7.0.0)\nRequirement already satisfied: polars in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.21.0)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics)\n  Downloading ultralytics_thop-2.0.17-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.6.15)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.0->ultralytics) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\nDownloading ultralytics-8.3.197-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.17-py3-none-any.whl (28 kB)\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.197 ultralytics-thop-2.0.17\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom ultralytics import YOLO\nimport os\nimport glob\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T00:38:54.077649Z","iopub.execute_input":"2025-09-10T00:38:54.077959Z","iopub.status.idle":"2025-09-10T00:38:58.263021Z","shell.execute_reply.started":"2025-09-10T00:38:54.077925Z","shell.execute_reply":"2025-09-10T00:38:58.262397Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file ✅ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Official Facebook ConvNext Implementation\nclass DropPath(nn.Module):\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        if self.drop_prob == 0. or not self.training:\n            return x\n        keep_prob = 1 - self.drop_prob\n        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n        random_tensor.floor_()\n        output = x.div(keep_prob) * random_tensor\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T00:38:58.263679Z","iopub.execute_input":"2025-09-10T00:38:58.264082Z","iopub.status.idle":"2025-09-10T00:38:58.269522Z","shell.execute_reply.started":"2025-09-10T00:38:58.264064Z","shell.execute_reply":"2025-09-10T00:38:58.268660Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(normalized_shape))\n        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n        self.eps = eps\n        self.data_format = data_format\n        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n            raise NotImplementedError \n        self.normalized_shape = (normalized_shape, )\n    \n    def forward(self, x):\n        if self.data_format == \"channels_last\":\n            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n        elif self.data_format == \"channels_first\":\n            u = x.mean(1, keepdim=True)\n            s = (x - u).pow(2).mean(1, keepdim=True)\n            x = (x - u) / torch.sqrt(s + self.eps)\n            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n            return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T00:38:58.270298Z","iopub.execute_input":"2025-09-10T00:38:58.270505Z","iopub.status.idle":"2025-09-10T00:38:58.285452Z","shell.execute_reply.started":"2025-09-10T00:38:58.270479Z","shell.execute_reply":"2025-09-10T00:38:58.284893Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class Block(nn.Module):\n    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n        super().__init__()\n        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)\n        self.norm = LayerNorm(dim, eps=1e-6)\n        self.pwconv1 = nn.Linear(dim, 4 * dim)\n        self.act = nn.GELU()\n        self.pwconv2 = nn.Linear(4 * dim, dim)\n        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), \n                                    requires_grad=True) if layer_scale_init_value > 0 else None\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        input = x\n        x = self.dwconv(x)\n        x = x.permute(0, 2, 3, 1)  # (N, C, H, W) -> (N, H, W, C)\n        x = self.norm(x)\n        x = self.pwconv1(x)\n        x = self.act(x)\n        x = self.pwconv2(x)\n        if self.gamma is not None:\n            x = self.gamma * x\n        x = x.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n        x = input + self.drop_path(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T00:38:58.286073Z","iopub.execute_input":"2025-09-10T00:38:58.286253Z","iopub.status.idle":"2025-09-10T00:38:58.299243Z","shell.execute_reply.started":"2025-09-10T00:38:58.286238Z","shell.execute_reply":"2025-09-10T00:38:58.298702Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\nclass ConvNeXt(nn.Module):\n    def __init__(self, in_chans=3, num_classes=1000, \n                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0., \n                 layer_scale_init_value=1e-6, head_init_scale=1.):\n        super().__init__()\n\n        self.downsample_layers = nn.ModuleList()\n        stem = nn.Sequential(\n            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n        )\n        self.downsample_layers.append(stem)\n        \n        for i in range(3):\n            downsample_layer = nn.Sequential(\n                    LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),\n            )\n            self.downsample_layers.append(downsample_layer)\n\n        self.stages = nn.ModuleList()\n        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] \n        cur = 0\n        for i in range(4):\n            stage = nn.Sequential(\n                *[Block(dim=dims[i], drop_path=dp_rates[cur + j], \n                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\n            )\n            self.stages.append(stage)\n            cur += depths[i]\n\n        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)\n        self.head = nn.Linear(dims[-1], num_classes)\n\n        self.apply(self._init_weights)\n        self.head.weight.data.mul_(head_init_scale)\n        self.head.bias.data.mul_(head_init_scale)\n\n    def _init_weights(self, m):\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            nn.init.trunc_normal_(m.weight, std=.02)\n            nn.init.constant_(m.bias, 0)\n\n    def forward_features(self, x):\n        features = []\n        for i in range(4):\n            x = self.downsample_layers[i](x)\n            x = self.stages[i](x)\n            features.append(x)\n        return features\n\n    def forward(self, x):\n        features = self.forward_features(x)\n        x = self.norm(features[-1].mean([-2, -1]))  # global average pooling\n        x = self.head(x)\n        return x\n\ndef convnext_tiny(pretrained=False, weights_path=None, **kwargs):\n    \"\"\"ConvNext Tiny model\"\"\"\n    model = ConvNeXt(depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], **kwargs)\n    \n    if pretrained and weights_path:\n        print(f\"🔄 Loading pretrained weights from {weights_path}\")\n        checkpoint = torch.load(weights_path, map_location=\"cpu\")\n        \n        # Handle different checkpoint formats\n        if \"model\" in checkpoint:\n            state_dict = checkpoint[\"model\"]\n        else:\n            state_dict = checkpoint\n            \n        model.load_state_dict(state_dict)\n        print(\"✅ Pretrained weights loaded successfully!\")\n    elif pretrained:\n        print(\"⚠️ Pretrained=True but no weights_path provided\")\n        \n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T00:38:58.301400Z","iopub.execute_input":"2025-09-10T00:38:58.301591Z","iopub.status.idle":"2025-09-10T00:38:58.318966Z","shell.execute_reply.started":"2025-09-10T00:38:58.301576Z","shell.execute_reply":"2025-09-10T00:38:58.318306Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class ConvNextBackbone(nn.Module):\n    \"\"\"ConvNext backbone optimized for YOLO integration\"\"\"\n    \n    def __init__(self, weights_path=None, pretrained=True):\n        super().__init__()\n        \n        # Create ConvNext Tiny with official architecture\n        self.convnext = convnext_tiny(pretrained=pretrained, weights_path=weights_path)\n        \n        # Remove classification head (we only need features)\n        self.convnext.norm = nn.Identity()\n        self.convnext.head = nn.Identity()\n        \n        # ConvNext Tiny feature dimensions: [96, 192, 384, 768]\n        self.feature_dims = [192, 384, 768]  # Last 3 stages for multi-scale detection\n        target_dims = [256, 512, 1024]       # YOLO expected channels\n        \n        # Feature adaptation layers\n        self.adapters = nn.ModuleList([\n            nn.Sequential(\n                nn.Conv2d(in_dim, out_dim, 1, bias=False),\n                nn.BatchNorm2d(out_dim),\n                nn.ReLU(inplace=True)\n            ) for in_dim, out_dim in zip(self.feature_dims, target_dims)\n        ])\n        \n        print(\"✅ ConvNext Tiny backbone ready for YOLO integration\")\n        print(f\"📊 Feature channels: {self.feature_dims} → {target_dims}\")\n\n    def forward(self, x):\n        # Extract multi-scale features from ConvNext\n        features = self.convnext.forward_features(x)\n        \n        # Take last 3 feature maps: [Stage1: 192ch, Stage2: 384ch, Stage3: 768ch]\n        selected_features = features[-3:]\n        \n        # Adapt to YOLO expected channels\n        adapted_features = []\n        for feat, adapter in zip(selected_features, self.adapters):\n            adapted_feat = adapter(feat)\n            adapted_features.append(adapted_feat)\n        \n        return adapted_features\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T00:38:58.319774Z","iopub.execute_input":"2025-09-10T00:38:58.320042Z","iopub.status.idle":"2025-09-10T00:38:58.337189Z","shell.execute_reply.started":"2025-09-10T00:38:58.320020Z","shell.execute_reply":"2025-09-10T00:38:58.336499Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\ndef download_convnext_weights(model_name=\"convnext_tiny_1k\"):\n    \"\"\"Download ConvNext weights if not present\"\"\"\n    model_urls = {\n        \"convnext_tiny_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\",\n        \"convnext_tiny_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth\",\n    }\n    \n    weights_file = f\"{model_name}.pth\"\n    \n    if os.path.exists(weights_file):\n        print(f\"✅ Found existing weights: {weights_file}\")\n        return weights_file\n    \n    if model_name in model_urls:\n        print(f\"📥 Downloading {model_name} weights...\")\n        checkpoint = torch.hub.load_state_dict_from_url(\n            url=model_urls[model_name], \n            map_location=\"cpu\", \n            file_name=weights_file\n        )\n        # Save to current directory for future use\n        torch.save(checkpoint, weights_file)\n        print(f\"✅ Downloaded and saved: {weights_file}\")\n        return weights_file\n    else:\n        print(f\"❌ Unknown model: {model_name}\")\n        return None\n\ndef train_model(train_dir, val_dir, epochs=50, batch_size=8, device='cuda'):\n    print(\"🔥 Training ConvNext-YOLO Fire Detection\")\n    print(\"=\" * 50)\n    \n    # Download ConvNext weights\n    weights_path = download_convnext_weights(\"convnext_tiny_1k\")\n    \n    # Create datasets\n    train_dataset = FireDataset(f\"{train_dir}/images\", f\"{train_dir}/labels\")\n    val_dataset = FireDataset(f\"{val_dir}/images\", f\"{val_dir}/labels\")\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    \n    print(f\"📊 Dataset: Train={len(train_dataset)}, Val={len(val_dataset)}\")\n    \n    # Create model with pretrained ConvNext\n    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n    model = ConvNextYOLO(\n        yolo_path=\"yolov8n.pt\",\n        num_classes=2,\n        convnext_weights_path=weights_path\n    ).to(device)\n    \n    # Training setup - different LR for backbone vs head\n    backbone_params = list(model.convnext_backbone.parameters())\n    head_params = list(model.neck.parameters()) + list(model.head.parameters())\n    \n    optimizer = torch.optim.AdamW([\n        {'params': backbone_params, 'lr': 1e-5, 'weight_decay': 1e-4},  # Lower LR for pretrained\n        {'params': head_params, 'lr': 1e-3, 'weight_decay': 1e-4}       # Higher LR for new layers\n    ])\n    \n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n    \n    best_loss = float('inf')\n    \n    print(f\"\\n🚀 Starting training on {device}\")\n    print(f\"🎯 Epochs: {epochs}, Batch size: {batch_size}\")\n    \n    for epoch in range(epochs):\n        # Training phase\n        model.train()\n        train_loss = 0\n        \n        for batch_idx, images in enumerate(train_loader):\n            images = images.to(device)\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(images)\n            \n            # Simple loss function that works with any output format\n            if isinstance(outputs, list):\n                # Handle list of outputs (multi-scale)\n                loss = sum(torch.mean(torch.abs(out)) for out in outputs)\n            elif isinstance(outputs, torch.Tensor):\n                # Handle single tensor output  \n                loss = torch.mean(torch.abs(outputs))\n            else:\n                # Fallback\n                loss = torch.tensor(0.1, requires_grad=True).to(device)\n            \n            # Backward pass\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            \n            if batch_idx % 10 == 0:\n                print(f\"Epoch {epoch+1}/{epochs} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n        \n        # Calculate average loss\n        avg_train_loss = train_loss / len(train_loader)\n        scheduler.step(avg_train_loss)\n        \n        # Save best model\n        if avg_train_loss < best_loss:\n            best_loss = avg_train_loss\n            torch.save({\n                'epoch': epoch + 1,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': avg_train_loss,\n                'convnext_weights_path': weights_path\n            }, 'best_convnext_yolo_fire.pth')\n            print(f\"✅ Best model saved! Loss: {avg_train_loss:.4f}\")\n        \n        print(f\"Epoch {epoch+1} completed | Avg Loss: {avg_train_loss:.4f}\")\n        print(\"-\" * 50)\n    \n    print(\"🎉 Training completed!\")\n    print(f\"💾 Best model saved as: best_convnext_yolo_fire.pth\")\n\ndef test_backbone():\n    \"\"\"Test ConvNext backbone loading\"\"\"\n    print(\"🧪 Testing ConvNext backbone...\")\n    \n    # Download weights\n    weights_path = download_convnext_weights(\"convnext_tiny_1k\")\n    \n    # Create backbone\n    backbone = ConvNextBackbone(weights_path=weights_path, pretrained=True)\n    \n    # Test forward pass\n    test_input = torch.randn(1, 3, 640, 640)\n    with torch.no_grad():\n        features = backbone(test_input)\n        \n    print(f\"✅ Backbone test successful!\")\n    print(f\"Input shape: {test_input.shape}\")\n    for i, feat in enumerate(features):\n        print(f\"Feature {i+1}: {feat.shape}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-10T00:38:58.337929Z","iopub.execute_input":"2025-09-10T00:38:58.338194Z","iopub.status.idle":"2025-09-10T00:38:58.356103Z","shell.execute_reply.started":"2025-09-10T00:38:58.338170Z","shell.execute_reply":"2025-09-10T00:38:58.355510Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom ultralytics import YOLO\n\n\nclass WorkingConvNextYOLO(nn.Module):\n    \"\"\"Working YOLO model with ConvNeXt backbone - fixed iteration issues\"\"\"\n    \n    def __init__(self, yolo_path=\"yolov8n.pt\", num_classes=2, convnext_weights_path=None, freeze_backbone=True, hyp=None):\n        super().__init__()\n        self.hyp = hyp\n        self.args = SimpleNamespace(hyp=self.hyp)\n        print(f\"Creating Working ConvNext-YOLO...\")\n        \n        # Load YOLO and get the underlying model\n        yolo_wrapper = YOLO(\"yolov8n.pt\")\n        self.yolo = yolo_wrapper.model  # This is the DetectionModel\n        self.head = self.yolo.model[-1]\n        self.stride = self.head.stride \n        print(f\"Original YOLO type: {type(self.yolo)}\")\n        \n        # Access the model's sequential layers properly\n        if hasattr(self.yolo, 'model'):\n            model_layers = self.yolo.model  # This should be the sequential part\n            print(f\"Model layers type: {type(model_layers)}\")\n            print(f\"Number of layers: {len(model_layers) if hasattr(model_layers, '__len__') else 'Unknown'}\")\n        \n        # Replace backbone\n        print(\"Replacing backbone with ConvNeXt...\")\n        #original_backbone = self.yolo.backbone\n        self.yolo.backbone = ConvNextBackbone(\n            weights_path=convnext_weights_path,\n            pretrained=True if convnext_weights_path else False\n        )\n        print(\"swapped\")\n        \n        # Freeze backbone if requested\n        if freeze_backbone:\n            print(\"Freezing ConvNeXt backbone...\")\n            for param in self.yolo.backbone.parameters():\n                param.requires_grad = False\n        \n        # Update detection head\n        self._update_detection_head(num_classes)\n        \n        # Ensure neck and head are trainable\n        self._ensure_trainable_components()\n        \n        self._print_summary()\n        \n        self.model = nn.Sequential(\n            *list(self.yolo.model[:-1]),  # backbone\n            self.yolo.model[-1]           # detection head\n        )\n        self.model.args=self.args \n    \n    def _update_detection_head(self, num_classes):\n        \"\"\"Update detection head for new classes\"\"\"\n        print(f\"Updating detection head: -> {num_classes} classes\")\n        \n        # Find the Detect layer - it's usually the last layer in the model\n        detect_layer = None\n        if hasattr(self.yolo, 'model'):\n            # Try to get the last layer\n            if hasattr(self.yolo.model, '__getitem__'):\n                detect_layer = self.yolo.model[-1]\n            else:\n                # Fallback: search through all modules\n                for module in self.yolo.modules():\n                    if hasattr(module, 'nc') and hasattr(module, 'cv2'):\n                        detect_layer = module\n                        break\n        \n        if detect_layer is None:\n            print(\"Warning: Could not find detection layer!\")\n            return\n        \n        print(f\"Found detection layer: {type(detect_layer)}\")\n        \n        # Update detection layer properties\n        old_nc = getattr(detect_layer, 'nc', 80)\n        detect_layer.nc = num_classes\n        detect_layer.no = num_classes + detect_layer.reg_max * 4\n        \n        # Update classification heads (cv2)\n        if hasattr(detect_layer, 'cv2'):\n            for i, cv2_layer in enumerate(detect_layer.cv2):\n                if hasattr(cv2_layer, '__getitem__') and len(cv2_layer) > 0:\n                    # Get the last conv layer in the cv2 sequence\n                    last_conv = cv2_layer[-1]\n                    if isinstance(last_conv, nn.Conv2d):\n                        in_channels = last_conv.in_channels\n                        # Replace with new conv layer for correct number of classes\n                        cv2_layer[-1] = nn.Conv2d(in_channels, num_classes, 1, bias=True)\n                        print(f\"  Updated cv2[{i}]: {in_channels} -> {num_classes}\")\n        \n        # Update regression heads (cv3) - these stay the same\n        if hasattr(detect_layer, 'cv3'):\n            for i, cv3_layer in enumerate(detect_layer.cv3):\n                if hasattr(cv3_layer, '__getitem__') and len(cv3_layer) > 0:\n                    last_conv = cv3_layer[-1]\n                    if isinstance(last_conv, nn.Conv2d):\n                        in_channels = last_conv.in_channels\n                        cv3_layer[-1] = nn.Conv2d(in_channels, 4 * detect_layer.reg_max, 1, bias=True)\n                        print(f\"  Updated cv3[{i}]: {in_channels} -> {4 * detect_layer.reg_max}\")\n    \n    def _ensure_trainable_components(self):\n        \"\"\"Ensure neck and head components are trainable\"\"\"\n        print(\"Ensuring neck and head are trainable...\")\n        \n        # Method 1: Unfreeze all non-backbone parameters\n        for name, module in self.yolo.named_children():\n            if name != 'backbone':\n                for param in module.parameters():\n                    param.requires_grad = True\n        \n        # Method 2: Specifically unfreeze detection layer\n        for module in self.yolo.modules():\n            if hasattr(module, 'nc') and hasattr(module, 'cv2'):  # This is Detect layer\n                for param in module.parameters():\n                    param.requires_grad = True\n                break\n        \n        # Method 3: If model has sequential structure, unfreeze specific indices\n        if hasattr(self.yolo, 'model') and hasattr(self.yolo.model, '__getitem__'):\n            try:\n                # Typical YOLOv8 structure: backbone is index 0, neck/head are later indices\n                model_layers = self.yolo.model\n                for i in range(1, len(model_layers)):  # Skip index 0 (backbone)\n                    for param in model_layers[i].parameters():\n                        param.requires_grad = True\n            except Exception as e:\n                print(f\"Could not iterate through model layers: {e}\")\n    \n    def _print_summary(self):\n        \"\"\"Print parameter summary\"\"\"\n        total = sum(p.numel() for p in self.parameters())\n        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n        frozen = total - trainable\n        \n        print(f\"\\nParameter Summary:\")\n        print(f\"  Total: {total:,} ({total/1e6:.1f}M)\")\n        print(f\"  Trainable: {trainable:,} ({trainable/1e6:.1f}M)\")\n        print(f\"  Frozen: {frozen:,} ({frozen/1e6:.1f}M)\")\n        print(f\"  Trainable Ratio: {trainable/total*100:.1f}%\")\n        \n        # Component breakdown\n        print(f\"\\nComponent Breakdown:\")\n        if hasattr(self.yolo, 'backbone'):\n            bb_total = sum(p.numel() for p in self.yolo.backbone.parameters())\n            bb_trainable = sum(p.numel() for p in self.yolo.backbone.parameters() if p.requires_grad)\n            print(f\"  Backbone: {bb_total:,} total, {bb_trainable:,} trainable\")\n        \n        # Count detection layer parameters\n        for module in self.yolo.modules():\n            if hasattr(module, 'nc') and hasattr(module, 'cv2'):\n                det_total = sum(p.numel() for p in module.parameters())\n                det_trainable = sum(p.numel() for p in module.parameters() if p.requires_grad)\n                print(f\"  Detection Head: {det_total:,} total, {det_trainable:,} trainable\")\n                break\n                \n    def init_criterion(self):\n        \"\"\"Initialize the loss function.\"\"\"\n        from ultralytics.utils.loss import v8DetectionLoss\n        self.criterion = v8DetectionLoss(self)  # reads hyp from self.args.hyp\n        return self.criterion\n        \n    def forward(self, x):\n        return self.yolo(x)\n\ndef test_working_model():\n    \"\"\"Test the working model\"\"\"\n    print(\"Testing Working ConvNext-YOLO...\")\n    \n    try:\n        # Download weights\n        weights_path = download_convnext_weights(\"convnext_tiny_1k\")\n        \n        # Create model\n        model = WorkingConvNextYOLO(\n            yolo_path=\"yolov8n.pt\",\n            num_classes=2,\n            convnext_weights_path=weights_path,\n            freeze_backbone=True\n        )\n        \n        # Test forward pass\n        print(\"\\nTesting forward pass...\")\n        test_input = torch.randn(1, 3, 640, 640)\n        \n        if torch.cuda.is_available():\n            model = model.cuda()\n            test_input = test_input.cuda()\n        \n        model.eval()\n        with torch.no_grad():\n            output = model(test_input)\n        \n        print(\"Forward pass successful!\")\n        \n        # Analyze output\n        if isinstance(output, (list, tuple)):\n            print(f\"Output: {len(output)} elements\")\n            for i, out in enumerate(output):\n                if hasattr(out, 'shape'):\n                    print(f\"  [{i}]: {out.shape}\")\n                else:\n                    print(f\"  [{i}]: {type(out)}\")\n        else:\n            print(f\"Output shape: {output.shape}\")\n        \n        return model\n        \n    except Exception as e:\n        print(f\"Error: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None\n\ndef debug_yolo_structure(yolo_path=\"yolov8n.pt\"):\n    \"\"\"Debug YOLO model structure\"\"\"\n    print(\"Debugging YOLO structure...\")\n    \n    yolo = YOLO(yolo_path)\n    model = yolo.model\n    \n    print(f\"YOLO wrapper type: {type(yolo)}\")\n    print(f\"Model type: {type(model)}\")\n    print(f\"Model attributes: {dir(model)}\")\n    \n    if hasattr(model, 'model'):\n        inner_model = model.model\n        print(f\"Inner model type: {type(inner_model)}\")\n        if hasattr(inner_model, '__len__'):\n            print(f\"Inner model length: {len(inner_model)}\")\n            for i, layer in enumerate(inner_model):\n                print(f\"  Layer {i}: {type(layer).__name__}\")\n                if i > 10:  # Don't print too many\n                    print(f\"  ... and {len(inner_model) - i - 1} more layers\")\n                    break\n    \n    if hasattr(model, 'backbone'):\n        print(f\"Backbone type: {type(model.backbone)}\")\n    \n    # Look for detection layer\n    for i, module in enumerate(model.modules()):\n        if hasattr(module, 'nc') and hasattr(module, 'cv2'):\n            print(f\"Found Detect layer at module {i}: nc={module.nc}\")\n            break\n\n# Simple fix for your existing code\ndef fix_your_convnext_yolo():\n    \"\"\"Fix for your existing ConvNextYOLO class\"\"\"\n    print(\"Quick fix for existing code...\")\n    \n    # The issue is in your __init__ method where you do:\n    # for i, module in enumerate(original_model):\n    \n    # Replace that section with:\n    code_fix = '''\n    # Instead of:\n    # for i, module in enumerate(original_model):\n    \n    # Use this:\n    if hasattr(self.yolo, 'model') and hasattr(self.yolo.model, '__getitem__'):\n        model_layers = self.yolo.model\n        for i in range(len(model_layers)):\n            module = model_layers[i]\n            # Your existing logic here\n    else:\n        # Fallback for non-iterable models\n        print(\"Warning: Model is not directly iterable\")\n    '''\n    \n    print(code_fix)\n\nif __name__ == \"__main__\":\n    # Debug first\n    debug_yolo_structure()\n    \n    print(\"\\n\" + \"=\"*60)\n    \n    # Test working model\n    model = test_working_model()\n    \n    if model:\n        # Run parameter analysis\n        print(\"yes\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T00:38:58.356794Z","iopub.execute_input":"2025-09-10T00:38:58.357020Z","iopub.status.idle":"2025-09-10T00:39:00.185126Z","shell.execute_reply.started":"2025-09-10T00:38:58.357003Z","shell.execute_reply":"2025-09-10T00:39:00.184369Z"}},"outputs":[{"name":"stdout","text":"Debugging YOLO structure...\n\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 16.7MB/s 0.4s.3s<0.2s2s\nYOLO wrapper type: <class 'ultralytics.models.yolo.model.YOLO'>\nModel type: <class 'ultralytics.nn.tasks.DetectionModel'>\nModel attributes: ['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_clip_augmented', '_compiled_call_impl', '_descale_pred', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_predict_augment', '_predict_once', '_profile_one_layer', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'args', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'fuse', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'info', 'init_criterion', 'inplace', 'ipu', 'is_fused', 'load', 'load_state_dict', 'loss', 'model', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'names', 'nc', 'parameters', 'predict', 'pt_path', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'save', 'set_extra_state', 'set_submodule', 'share_memory', 'state_dict', 'stride', 'task', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'yaml', 'yaml_file', 'zero_grad']\nInner model type: <class 'torch.nn.modules.container.Sequential'>\nInner model length: 23\n  Layer 0: Conv\n  Layer 1: Conv\n  Layer 2: C2f\n  Layer 3: Conv\n  Layer 4: C2f\n  Layer 5: Conv\n  Layer 6: C2f\n  Layer 7: Conv\n  Layer 8: C2f\n  Layer 9: SPPF\n  Layer 10: Upsample\n  Layer 11: Concat\n  ... and 11 more layers\nFound Detect layer at module 172: nc=80\n\n============================================================\nTesting Working ConvNext-YOLO...\n📥 Downloading convnext_tiny_1k weights...\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny_1k.pth\n100%|██████████| 109M/109M [00:00<00:00, 408MB/s] \n","output_type":"stream"},{"name":"stdout","text":"✅ Downloaded and saved: convnext_tiny_1k.pth\nError: name 'SimpleNamespace' is not defined\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_36/4101751676.py\", line 180, in test_working_model\n    model = WorkingConvNextYOLO(\n            ^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_36/4101751676.py\", line 12, in __init__\n    self.args = SimpleNamespace(hyp=self.hyp)\n                ^^^^^^^^^^^^^^^\nNameError: name 'SimpleNamespace' is not defined\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\n\n\nyaml_content = \"\"\"\n    path: /kaggle/input/home-fire-dataset\n    train: train/images\n    val: val/images\n    test: test/images\n    \n    nc: 2\n    names: ['fire', 'smoke']\n    \"\"\"\n\nwith open(\"/kaggle/working/home-fire.yaml\", \"w\") as f:\n    f.write(yaml_content)\ndef train_convnext_yolo(model_path='yolov8n.pt', data_yaml='/kaggle/working/home-fire.yaml', epochs=100):\n    \"\"\"Train ConvNeXt-YOLO model\"\"\"\n    \n    # Initialize your custom model\n    weights_path = download_convnext_weights(\"convnext_tiny_1k\")\n    # Create model with frozen backbone\n    model = ConvNextYOLO(yolo_path=\"/kaggle/working/yolov8n.pt\", num_classes=2, convnext_weights_path=weights_path, freeze_backbone=True)\n\n    \n    # Training configuration\n    train_config = {\n        'data': data_yaml,\n        'epochs': epochs,\n        'imgsz': 640,\n        'batch': 16,  # Adjust based on your GPU memory\n        'lr0': 0.01,  # Initial learning rate\n        'lrf': 0.01,  # Final learning rate\n        'momentum': 0.937,\n        'weight_decay': 0.0005,\n        'warmup_epochs': 3,\n        'warmup_momentum': 0.8,\n        'warmup_bias_lr': 0.1,\n        'box': 7.5,  # Box loss gain\n        'cls': 0.5,  # Classification loss gain\n        'dfl': 1.5,  # DFL loss gain\n        'save': True,\n        'save_period': 10,  # Save checkpoint every 10 epochs\n        'cache': False,  # Set to True if you have enough RAM\n        'device': '0',  # GPU device, set to 'cpu' if no GPU\n        'workers': 8,\n        'project': 'runs/detect',\n        'name': 'convnext_yolo',\n        'exist_ok': True,\n        'pretrained': False,  # We're using custom model\n        'optimizer': 'AdamW',\n        'verbose': True,\n        'seed': 0,\n        'deterministic': True,\n        'single_cls': False,\n        'rect': False,\n        'cos_lr': True,\n        'close_mosaic': 10,  # Disable mosaic augmentation for last 10 epochs\n        'resume': False,\n        'amp': True,  # Automatic Mixed Precision\n        'fraction': 1.0,\n        'profile': False,\n        'freeze': None,  # We handle freezing in our model\n    }\n    \n    # Create trainer\n    from ultralytics.models.yolo.detect import DetectionTrainer\n    \n    trainer = DetectionTrainer(overrides=train_config)\n    trainer.model = model  # Use our custom model\n    \n    # Start training\n    trainer.train()\n    \n    print(\"🎉 Training completed!\")\n    return trainer\n\n# 3. ALTERNATIVE: SIMPLER TRAINING WITH ULTRALYTICS\ndef train_simple_method():\n\n    \n    print(\"Simple Integration Approach\")\n    print(\"=\" * 50)\n    \n    # Setup\n    weights_path = download_convnext_weights(\"convnext_tiny_1k\")\n    \n    # Create a custom model configuration\n    def create_custom_model():\n        # Load base YOLO\n        yolo = YOLO(\"yolov8n.pt\")\n        \n        # Replace backbone\n        yolo.model.model[0] = ConvNextBackbone(\n            weights_path=weights_path,\n            pretrained=True\n        )\n        \n        # Freeze backbone\n        for param in yolo.model.model[0].parameters():\n            param.requires_grad = False\n        \n        return yolo\n    \n    # Create and train\n    model = create_custom_model()\n    \n    # Train with standard YOLO interface\n    results = model.train(\n        data='/kaggle/working/home-fire.yaml',\n        epochs=50,\n        imgsz=640,\n        batch=8,\n        lr0=0.001,\n        optimizer='AdamW',\n        project='convnext_yolo',\n        name='fire_detection',\n        device=0 if torch.cuda.is_available() else 'cpu',\n        verbose=True\n    )\n    \n    return results, model \n    \n    print(\"Simple Integration Approach\")\n    print(\"=\" * 50)\n    \n    # Setup\n    setup_yaml_config()\n    weights_path = download_convnext_weights(\"convnext_tiny_1k\")\n    \n    # Create a custom model configuration\n    def create_custom_model():\n        # Load base YOLO\n        yolo = YOLO(\"yolov8n.pt\")\n        \n        # Replace backbone\n        yolo.model.model[0] = ConvNextBackbone(\n            weights_path=weights_path,\n            pretrained=True\n        )\n        \n        # Freeze backbone\n        for param in yolo.model.model[0].parameters():\n            param.requires_grad = False\n        \n        return yolo\n    \n    # Create and train\n    model = create_custom_model()\n    \n    # Train with standard YOLO interface\n    results = model.train(\n        data='/kaggle/working/home-fire.yaml',\n        epochs=50,\n        imgsz=640,\n        batch=8,\n        lr0=0.001,\n        optimizer='AdamW',\n        project='convnext_yolo',\n        name='fire_detection',\n        device=0 if torch.cuda.is_available() else 'cpu',\n        verbose=True\n    )\n    \n    return results, model\n\n# 4. PROGRESSIVE TRAINING STRATEGY\ndef progressive_training():\n    \"\"\"Alternative: Manual training loop with custom components\"\"\"\n    \n    print(\"Manual training approach...\")\n    \n    # Create model\n    weights_path = download_convnext_weights(\"convnext_tiny_1k\")\n    model = WorkingConvNextYOLO(\n        yolo_path=\"yolov8n.pt\",\n        num_classes=2,\n        convnext_weights_path=weights_path,\n        freeze_backbone=True\n    )\n    \n    # Load dataset using YOLO's data loading\n    from ultralytics.data import build_dataloader\n    from ultralytics.cfg import get_cfg\n    \n    # Load config\n    cfg = get_cfg(overrides={'data': '/kaggle/working/home-fire.yaml'})\n    \n    # Create data loaders\n    train_loader = build_dataloader(\n        dataset_path='/kaggle/input/home-fire-dataset/train',\n        imgsz=640,\n        batch_size=8,\n        stride=32,\n        single_cls=False,\n        hyp=cfg,\n        augment=True,\n        cache=False,\n        workers=2,\n        shuffle=True,\n        rank=-1\n    )\n    \n    val_loader = build_dataloader(\n        dataset_path='/kaggle/input/home-fire-dataset/val',\n        imgsz=640,\n        batch_size=8,\n        stride=32,\n        single_cls=False,\n        hyp=cfg,\n        augment=False,\n        cache=False,\n        workers=2,\n        shuffle=False,\n        rank=-1\n    )\n    \n    # Setup training\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    \n    # Optimizer with different LRs\n    backbone_params = []\n    other_params = []\n    \n    for name, param in model.named_parameters():\n        if 'backbone' in name:\n            backbone_params.append(param)\n        else:\n            other_params.append(param)\n    \n    optimizer = torch.optim.AdamW([\n        {'params': backbone_params, 'lr': 1e-5},  # Lower for pretrained\n        {'params': other_params, 'lr': 1e-3}     # Higher for new layers\n    ], weight_decay=1e-4)\n    \n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n    \n    # Training loop\n    num_epochs = 50\n    best_loss = float('inf')\n    \n    for epoch in range(num_epochs):\n        model.train()\n        epoch_loss = 0\n        \n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        \n        for batch_idx, batch in enumerate(train_loader):\n            images = batch['img'].to(device).float() / 255.0\n            targets = batch['batch_idx'], batch['cls'], batch['bboxes']\n            \n            optimizer.zero_grad()\n            \n            # Forward pass\n            predictions = model(images)\n            \n            # Calculate loss (you'll need to implement proper YOLO loss)\n            # For now, using a simple placeholder\n            if isinstance(predictions, (list, tuple)):\n                loss = sum(pred.mean() for pred in predictions if pred.numel() > 0)\n            else:\n                loss = predictions.mean()\n            \n            # Backward pass\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n            \n            if batch_idx % 20 == 0:\n                print(f\"  Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n        \n        scheduler.step()\n        avg_loss = epoch_loss / len(train_loader)\n        \n        # Save best model\n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            torch.save({\n                'epoch': epoch + 1,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': avg_loss,\n            }, 'best_convnext_yolo_manual.pth')\n            print(f\"Best model saved! Loss: {avg_loss:.4f}\")\n        \n        print(f\"Epoch {epoch+1} completed, Avg Loss: {avg_loss:.4f}\")\n\n# 5. VALIDATION AND TESTING\ndef validate_model(model_path, data_yaml='/kaggle/working/home-fire.yaml'):\n    \"\"\"Validate trained model\"\"\"\n    \n    model = YOLO(model_path)\n    results = model.val(\n        data=data_yaml,\n        imgsz=640,\n        batch=1,\n        conf=0.25,\n        iou=0.6,\n        device=0\n    )\n    \n    print(f\"mAP50: {results.box.map50}\")\n    print(f\"mAP50-95: {results.box.map}\")\n    \n    return results\n\n# 6. MAIN EXECUTION\nif __name__ == \"__main__\":\n    # Step 1: Setup dataset\n    data_yaml = '/kaggle/working/home-fire.yaml'\n    \n    # Step 2: Choose training method\n    print(\"Choose training method:\")\n    print(\"1. Custom ConvNeXt-YOLO training\")\n    print(\"2. Progressive training (recommended)\")\n    print(\"3. Simple method\")\n    \n    choice = input(\"Enter choice (1-3): \")\n    \n    if choice == \"1\":\n        trainer = train_convnext_yolo(data_yaml=data_yaml)\n    elif choice == \"2\":\n        progressive_training()\n    else:\n        results = train_simple_method()\n    \n    print(\"🎯 Training pipeline complete!\")\n\n                                                                            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T00:39:00.186146Z","iopub.execute_input":"2025-09-10T00:39:00.186421Z","iopub.status.idle":"2025-09-10T00:39:07.783355Z","shell.execute_reply.started":"2025-09-10T00:39:00.186398Z","shell.execute_reply":"2025-09-10T00:39:07.782333Z"}},"outputs":[{"name":"stdout","text":"Choose training method:\n1. Custom ConvNeXt-YOLO training\n2. Progressive training (recommended)\n3. Simple method\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter choice (1-3):  5\n"},{"name":"stdout","text":"Simple Integration Approach\n==================================================\n✅ Found existing weights: convnext_tiny_1k.pth\n🔄 Loading pretrained weights from convnext_tiny_1k.pth\n✅ Pretrained weights loaded successfully!\n✅ ConvNext Tiny backbone ready for YOLO integration\n📊 Feature channels: [192, 384, 768] → [256, 512, 1024]\nUltralytics 8.3.197 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/kaggle/working/home-fire.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=fire_detection, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=convnext_yolo, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/kaggle/working/convnext_yolo/fire_detection, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ━━━━━━━━━━━━ 755.1KB 3.4MB/s 0.2s 0.2s<0.0s\nOverriding model.yaml nc=80 with nc=2\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \nModel summary: 129 layers, 3,011,238 parameters, 3,011,222 gradients, 8.2 GFLOPs\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1133535449.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mprogressive_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_simple_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🎯 Training pipeline complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/1133535449.py\u001b[0m in \u001b[0;36mtrain_simple_method\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# Train with standard YOLO interface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     results = model.train(\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/kaggle/working/home-fire.yaml'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_smart_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trainer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resume\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# manually set model only if not resuming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/models/yolo/detect/train.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(self, cfg, weights, verbose)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDetectionModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"channels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, weights, verbose)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfirst_conv\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mupdated_csd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfirst_conv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst_conv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mcc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst_conv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcw\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0mc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'model.0.conv.weight'"],"ename":"KeyError","evalue":"'model.0.conv.weight'","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport numpy as np\nimport glob\nimport os\nimport cv2\nimport random\nfrom pathlib import Path\n\nimport os, glob, random\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nclass FireDetectionDataset(Dataset):\n    \"\"\"Custom dataset for fire detection (YOLO-style labels).\"\"\"\n    \n    def __init__(self, images_dir, labels_dir, img_size=640, augment=False):\n        self.img_size = img_size\n        self.augment = augment\n        \n        # Collect image paths\n        image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff']\n        self.image_paths = []\n        for ext in image_extensions:\n            self.image_paths.extend(glob.glob(os.path.join(images_dir, ext)))\n        \n        self.labels_dir = labels_dir\n        print(f\"Found {len(self.image_paths)} images in {images_dir}\")\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        \n        # --- Load image ---\n        image = cv2.imread(img_path)\n        if image is None:\n            image = np.array(Image.open(img_path).convert('RGB'))\n        else:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        h0, w0 = image.shape[:2]\n        image = cv2.resize(image, (self.img_size, self.img_size))\n        h, w = image.shape[:2]\n\n        # --- Load labels ---\n        targets = {\"boxes\": torch.empty((0, 4)), \"labels\": torch.empty((0,), dtype=torch.long)}\n        if self.labels_dir is not None:\n            label_file = os.path.splitext(os.path.basename(img_path))[0] + \".txt\"\n            label_path = os.path.join(self.labels_dir, label_file)\n            if os.path.exists(label_path):\n                boxes, labels = [], []\n                with open(label_path, \"r\") as f:\n                    for line in f.readlines():\n                        cls, x, y, bw, bh = map(float, line.strip().split())\n                        labels.append(int(cls))\n                        boxes.append([\n                            x * w, y * h,      # center x,y (scaled to resized image)\n                            bw * w, bh * h     # width, height (scaled)\n                        ])\n                if boxes:\n                    targets[\"boxes\"] = torch.tensor(boxes, dtype=torch.float32)\n                    targets[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n\n        # --- Augmentation ---\n        if self.augment:\n            if random.random() > 0.5:  # horizontal flip\n                image = cv2.flip(image, 1)\n                if targets[\"boxes\"].numel() > 0:\n                    targets[\"boxes\"][:, 0] = w - targets[\"boxes\"][:, 0]  # flip x center\n\n        # --- Final conversion ---\n        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n        \n        return image, targets\n\n\ndef train_working_convnext_yolo(dataset_path, epochs=30, batch_size=4, learning_rate=0.001):\n    \"\"\"Train the WorkingConvNextYOLO model\"\"\"\n    \n    print(\"Training WorkingConvNextYOLO Model\")\n    print(\"=\" * 50)\n    \n    # Device setup\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Download ConvNeXt weights\n    print(\"Downloading ConvNeXt weights...\")\n    weights_path = download_convnext_weights(\"convnext_tiny_1k\")\n    \n    # Create datasets\n    print(\"Creating datasets...\")\n    train_dataset, val_dataset = create_fire_datasets(dataset_path)\n    \n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=2,\n        pin_memory=True,\n        drop_last=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset, \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=2,\n        pin_memory=True,\n        drop_last=False\n    )\n    \n    print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n    \n    # Create model\n    print(\"Creating WorkingConvNextYOLO model...\")\n    model = WorkingConvNextYOLO(\n        yolo_path=\"yolov8n.pt\",\n        num_classes=2,  # fire and smoke\n        convnext_weights_path=weights_path,\n        freeze_backbone=True\n    )\n    \n    model = model.to(device)\n    model.train()\n    \n    # Setup optimizer with different learning rates\n    backbone_params = []\n    other_params = []\n    \n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            if 'backbone' in name.lower() or 'convnext' in name.lower():\n                backbone_params.append(param)\n            else:\n                other_params.append(param)\n    \n    # Optimizer with different learning rates\n    optimizer = torch.optim.AdamW([\n        {'params': backbone_params, 'lr': learning_rate * 0.1, 'weight_decay': 1e-4},  # Lower LR for backbone\n        {'params': other_params, 'lr': learning_rate, 'weight_decay': 1e-4}  # Higher LR for head\n    ])\n    \n    # Learning rate scheduler\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n    \n    # Training variables\n    best_loss = float('inf')\n    train_losses = []\n    val_losses = []\n    \n    print(f\"Starting training for {epochs} epochs...\")\n    print(f\"Batch size: {batch_size}, Learning rate: {learning_rate}\")\n    \n    # Training loop\n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n        print(\"-\" * 40)\n        \n        # Training phase\n        model.train()\n        epoch_train_loss = 0.0\n        num_train_batches = 0\n        \n        for batch_idx, images in enumerate(train_loader):\n            images = images.to(device, non_blocking=True)\n            \n            # Zero gradients\n            optimizer.zero_grad()\n            \n            try:\n                # Forward pass\n                outputs = model(images)\n                \n                # Compute loss (simplified loss function)\n                # In practice, you'd want to implement proper YOLO loss with ground truth labels\n                if isinstance(outputs, (list, tuple)):\n                    # Handle multiple outputs (typical for YOLO)\n                    loss = torch.tensor(0.0, requires_grad=True).to(device)\n                    for output in outputs:\n                        if torch.is_tensor(output) and output.requires_grad:\n                            # Simple loss based on output magnitude (placeholder)\n                            loss = loss + torch.mean(torch.abs(output)) * 0.1\n                else:\n                    # Single output\n                    loss = torch.mean(torch.abs(outputs)) * 0.1\n                \n                # Add regularization loss\n                l2_loss = torch.tensor(0.0).to(device)\n                for param in model.parameters():\n                    if param.requires_grad:\n                        l2_loss = l2_loss + torch.norm(param, 2) ** 2\n                \n                total_loss = loss + 1e-6 * l2_loss\n                \n                # Backward pass\n                total_loss.backward()\n                \n                # Gradient clipping\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n                \n                # Optimizer step\n                optimizer.step()\n                \n                epoch_train_loss += total_loss.item()\n                num_train_batches += 1\n                \n                # Print progress\n                if batch_idx % 20 == 0:\n                    print(f\"  Batch {batch_idx:3d}/{len(train_loader)} | Loss: {total_loss.item():.6f}\")\n                \n            except Exception as e:\n                print(f\"  Error in batch {batch_idx}: {str(e)}\")\n                continue\n        \n        # Calculate average training loss\n        if num_train_batches > 0:\n            avg_train_loss = epoch_train_loss / num_train_batches\n            train_losses.append(avg_train_loss)\n        else:\n            avg_train_loss = float('inf')\n            train_losses.append(avg_train_loss)\n        \n        # Validation phase\n        model.eval()\n        epoch_val_loss = 0.0\n        num_val_batches = 0\n        \n        with torch.no_grad():\n            for batch_idx, images in enumerate(val_loader):\n                images = images.to(device, non_blocking=True)\n                \n                try:\n                    outputs = model(images)\n                    \n                    # Validation loss (same as training loss)\n                    if isinstance(outputs, (list, tuple)):\n                        val_loss = torch.tensor(0.0).to(device)\n                        for output in outputs:\n                            if torch.is_tensor(output):\n                                val_loss = val_loss + torch.mean(torch.abs(output)) * 0.1\n                    else:\n                        val_loss = torch.mean(torch.abs(outputs)) * 0.1\n                    \n                    epoch_val_loss += val_loss.item()\n                    num_val_batches += 1\n                    \n                except Exception as e:\n                    print(f\"  Validation error in batch {batch_idx}: {str(e)}\")\n                    continue\n        \n        # Calculate average validation loss\n        if num_val_batches > 0:\n            avg_val_loss = epoch_val_loss / num_val_batches\n            val_losses.append(avg_val_loss)\n        else:\n            avg_val_loss = float('inf')\n            val_losses.append(avg_val_loss)\n        \n        # Update learning rate\n        scheduler.step()\n        current_lr = optimizer.param_groups[0]['lr']\n        \n        # Save best model\n        if avg_train_loss < best_loss:\n            best_loss = avg_train_loss\n            checkpoint = {\n                'epoch': epoch + 1,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'train_loss': avg_train_loss,\n                'val_loss': avg_val_loss,\n                'convnext_weights_path': weights_path\n            }\n            torch.save(checkpoint, 'best_working_convnext_yolo.pth')\n            print(f\"  *** Best model saved! Train Loss: {avg_train_loss:.6f}\")\n        \n        # Print epoch summary\n        print(f\"  Epoch Summary:\")\n        print(f\"    Train Loss: {avg_train_loss:.6f}\")\n        print(f\"    Val Loss:   {avg_val_loss:.6f}\")\n        print(f\"    Learning Rate: {current_lr:.8f}\")\n        print(f\"    Best Loss: {best_loss:.6f}\")\n    \n    print(\"\\nTraining completed!\")\n    print(f\"Best training loss: {best_loss:.6f}\")\n    print(\"Model saved as: best_working_convnext_yolo.pth\")\n    \n    return model, train_losses, val_losses\n\ndef load_trained_model(checkpoint_path='best_working_convnext_yolo.pth'):\n    \"\"\"Load a trained WorkingConvNextYOLO model\"\"\"\n    \n    print(f\"Loading trained model from {checkpoint_path}\")\n    \n    # Load checkpoint\n    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n    \n    # Create model\n    model = WorkingConvNextYOLO(\n        yolo_path=\"yolov8n.pt\",\n        num_classes=2,\n        convnext_weights_path=checkpoint.get('convnext_weights_path'),\n        freeze_backbone=True\n    )\n    \n    # Load state dict\n    model.load_state_dict(checkpoint['model_state_dict'])\n    \n    print(f\"Model loaded successfully!\")\n    print(f\"Trained for {checkpoint['epoch']} epochs\")\n    print(f\"Final train loss: {checkpoint['train_loss']:.6f}\")\n    \n    return model\n\ndef test_model_inference(model, test_image_path):\n    \"\"\"Test the trained model on a single image\"\"\"\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    model.eval()\n    \n    # Load and preprocess image\n    image = cv2.imread(test_image_path)\n    if image is None:\n        image = np.array(Image.open(test_image_path).convert('RGB'))\n    else:\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    # Resize and normalize\n    image = cv2.resize(image, (640, 640))\n    image_tensor = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n    image_tensor = image_tensor.unsqueeze(0).to(device)\n    \n    # Inference\n    with torch.no_grad():\n        outputs = model(image_tensor)\n    \n    print(f\"Model output type: {type(outputs)}\")\n    if isinstance(outputs, (list, tuple)):\n        print(f\"Number of outputs: {len(outputs)}\")\n        for i, out in enumerate(outputs):\n            if torch.is_tensor(out):\n                print(f\"  Output {i}: {out.shape}\")\n    else:\n        print(f\"Output shape: {outputs.shape}\")\n    \n    return outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T00:39:11.771242Z","iopub.execute_input":"2025-09-10T00:39:11.771739Z","iopub.status.idle":"2025-09-10T00:39:11.801009Z","shell.execute_reply.started":"2025-09-10T00:39:11.771715Z","shell.execute_reply":"2025-09-10T00:39:11.800313Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n\ndef bbox_ciou(pred_boxes, target_boxes, eps=1e-7):\n    \"\"\"CIoU between pred and target boxes (x,y,w,h).\"\"\"\n    # Convert to corners\n    pred_x1 = pred_boxes[:, 0] - pred_boxes[:, 2] / 2\n    pred_y1 = pred_boxes[:, 1] - pred_boxes[:, 3] / 2\n    pred_x2 = pred_boxes[:, 0] + pred_boxes[:, 2] / 2\n    pred_y2 = pred_boxes[:, 1] + pred_boxes[:, 3] / 2\n\n    targ_x1 = target_boxes[:, 0] - target_boxes[:, 2] / 2\n    targ_y1 = target_boxes[:, 1] - target_boxes[:, 3] / 2\n    targ_x2 = target_boxes[:, 0] + target_boxes[:, 2] / 2\n    targ_y2 = target_boxes[:, 1] + target_boxes[:, 3] / 2\n\n    # Intersection\n    inter_x1 = torch.max(pred_x1, targ_x1)\n    inter_y1 = torch.max(pred_y1, targ_y1)\n    inter_x2 = torch.min(pred_x2, targ_x2)\n    inter_y2 = torch.min(pred_y2, targ_y2)\n    inter_area = (inter_x2 - inter_x1).clamp(0) * (inter_y2 - inter_y1).clamp(0)\n\n    # Union\n    pred_area = (pred_x2 - pred_x1).clamp(0) * (pred_y2 - pred_y1).clamp(0)\n    targ_area = (targ_x2 - targ_x1).clamp(0) * (targ_y2 - targ_y1).clamp(0)\n    union = pred_area + targ_area - inter_area + eps\n    iou = inter_area / union\n\n    # Enclosing box\n    cw = torch.max(pred_x2, targ_x2) - torch.min(pred_x1, targ_x1)\n    ch = torch.max(pred_y2, targ_y2) - torch.min(pred_y1, targ_y1)\n    c2 = cw ** 2 + ch ** 2 + eps\n\n    # Center distance\n    rho2 = ((pred_boxes[:, 0] - target_boxes[:, 0]) ** 2 +\n            (pred_boxes[:, 1] - target_boxes[:, 1]) ** 2)\n\n    # Aspect ratio penalty\n    v = (4 / (math.pi ** 2)) * torch.pow(\n        torch.atan(target_boxes[:, 2] / (target_boxes[:, 3] + eps)) -\n        torch.atan(pred_boxes[:, 2] / (pred_boxes[:, 3] + eps)), 2\n    )\n    with torch.no_grad():\n        alpha = v / (v - iou + (1 + eps))\n\n    ciou = iou - (rho2 / c2 + alpha * v)\n    return ciou\n\n\n\nclass YOLOLoss(nn.Module):\n    def __init__(self, num_classes=1, box_gain=7.5, obj_gain=1.0, cls_gain=0.5, iou_threshold=0.1):\n        super().__init__()\n        self.num_classes = num_classes\n        self.box_gain = box_gain\n        self.obj_gain = obj_gain\n        self.cls_gain = cls_gain\n        self.iou_threshold = iou_threshold\n\n        self.bce_obj = nn.BCEWithLogitsLoss(reduction=\"mean\")\n        self.bce_cls = nn.BCEWithLogitsLoss(reduction=\"mean\")\n\n    def forward(self, preds, targets):\n        \"\"\"\n        preds: [B, 6, N] where 6 = (x,y,w,h,obj,cls)\n        targets: list of dicts, each with \"boxes\" [M,4] and \"labels\" [M]\n        \"\"\"\n        device = preds.device\n        B, _, N = preds.shape\n\n        # Split predictions\n        pred_boxes = preds[:, :4, :].permute(0, 2, 1)  # [B, N, 4]\n        pred_obj   = preds[:, 4, :]                    # [B, N]\n        pred_cls   = preds[:, 5:, :].permute(0, 2, 1)  # [B, N, num_classes]\n\n        total_lbox, total_lobj, total_lcls = 0.0, 0.0, 0.0\n\n        for i in range(B):\n            boxes_i  = pred_boxes[i]   # [N,4]\n            obj_i    = pred_obj[i]     # [N]\n            cls_i    = pred_cls[i]     # [N, num_classes]\n\n            targets_i = targets[i]\n            target_boxes  = targets_i[\"boxes\"].to(device)   # [M,4]\n            target_labels = targets_i[\"labels\"].to(device)  # [M]\n\n            M = target_boxes.shape[0]\n\n            if M == 0:\n                # No targets → all objectness = 0\n                total_lobj += self.bce_obj(obj_i, torch.zeros_like(obj_i)) * self.obj_gain\n                continue\n\n            # Match predictions to GT via IoU\n            ious = bbox_iou_matrix(boxes_i, target_boxes)  # [N, M]\n            best_iou, best_idx = ious.max(dim=1)           # [N]\n            pos_mask = best_iou > self.iou_threshold       # [N]\n\n            matched_boxes  = target_boxes[best_idx[pos_mask]]\n            matched_labels = target_labels[best_idx[pos_mask]]\n\n            # --- Box loss (CIoU) ---\n            if pos_mask.sum() > 0:\n                l_box = (1 - bbox_ciou(boxes_i[pos_mask], matched_boxes)).mean() * self.box_gain\n                l_cls = self.bce_cls(\n                    cls_i[pos_mask].squeeze(-1),   # [N,1] -> [N]\n                    matched_labels.float()         # [N]\n                ) * self.cls_gain\n\n            else:\n                l_box = torch.tensor(0., device=device)\n                l_cls = torch.tensor(0., device=device)\n\n            # --- Objectness loss ---\n            l_obj = self.bce_obj(obj_i, pos_mask.float()) * self.obj_gain\n\n            total_lbox += l_box\n            total_lobj += l_obj\n            total_lcls += l_cls\n\n        total = total_lbox + total_lobj + total_lcls\n\n        return total, {\n            \"lbox\": total_lbox.detach(),\n            \"lobj\": total_lobj.detach(),\n            \"lcls\": total_lcls.detach()\n        }\n\ndef bbox_iou_matrix(box1, box2, eps=1e-7):\n    \"\"\"Compute IoU between two sets of boxes: box1=[N,4], box2=[M,4]\"\"\"\n   \n    device = box1.device\n    box2 = box2.to(device)\n\n    # Convert to corners\n    box1_x1 = box1[:,0:1] - box1[:,2:3]/2\n    box1_y1 = box1[:,1:2] - box1[:,3:4]/2\n    box1_x2 = box1[:,0:1] + box1[:,2:3]/2\n    box1_y2 = box1[:,1:2] + box1[:,3:4]/2\n\n    box2_x1 = box2[:,0:1] - box2[:,2:3]/2\n    box2_y1 = box2[:,1:2] - box2[:,3:4]/2\n    box2_x2 = box2[:,0:1] + box2[:,2:3]/2\n    box2_y2 = box2[:,1:2] + box2[:,3:4]/2\n\n    # Intersection\n    inter_x1 = torch.max(box1_x1, box2_x1.T)\n    inter_y1 = torch.max(box1_y1, box2_y1.T)\n    inter_x2 = torch.min(box1_x2, box2_x2.T)\n    inter_y2 = torch.min(box1_y2, box2_y2.T)\n    inter_area = (inter_x2 - inter_x1).clamp(0) * (inter_y2 - inter_y1).clamp(0)\n\n    # Union\n    area1 = (box1_x2 - box1_x1) * (box1_y2 - box1_y1)\n    area2 = (box2_x2 - box2_x1) * (box2_y2 - box2_y1)\n    union = area1 + area2.T - inter_area + eps\n\n    return inter_area / union  # [N,M]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T00:39:18.090655Z","iopub.execute_input":"2025-09-10T00:39:18.091390Z","iopub.status.idle":"2025-09-10T00:39:18.108709Z","shell.execute_reply.started":"2025-09-10T00:39:18.091366Z","shell.execute_reply":"2025-09-10T00:39:18.107981Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import os, glob, random\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nclass FireDetectionDataset(Dataset):\n    \"\"\"Custom dataset for fire detection (YOLO-style labels).\"\"\"\n    \n    def __init__(self, images_dir, labels_dir, img_size=640, augment=False):\n        self.img_size = img_size\n        self.augment = augment\n        \n        # Collect image paths\n        image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff']\n        self.image_paths = []\n        for ext in image_extensions:\n            self.image_paths.extend(glob.glob(os.path.join(images_dir, ext)))\n        \n        self.labels_dir = labels_dir\n        print(f\"Found {len(self.image_paths)} images in {images_dir}\")\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        \n        # --- Load image ---\n        image = cv2.imread(img_path)\n        if image is None:\n            image = np.array(Image.open(img_path).convert('RGB'))\n        else:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        h0, w0 = image.shape[:2]\n        image = cv2.resize(image, (self.img_size, self.img_size))\n        h, w = image.shape[:2]\n\n        # --- Load labels ---\n        targets = {\"boxes\": torch.empty((0, 4)), \"labels\": torch.empty((0,), dtype=torch.long)}\n        if self.labels_dir is not None:\n            label_file = os.path.splitext(os.path.basename(img_path))[0] + \".txt\"\n            label_path = os.path.join(self.labels_dir, label_file)\n            if os.path.exists(label_path):\n                boxes, labels = [], []\n                with open(label_path, \"r\") as f:\n                    for line in f.readlines():\n                        cls, x, y, bw, bh = map(float, line.strip().split())\n                        labels.append(int(cls))\n                        boxes.append([\n                            x * w, y * h,      # center x,y (scaled to resized image)\n                            bw * w, bh * h     # width, height (scaled)\n                        ])\n                if boxes:\n                    targets[\"boxes\"] = torch.tensor(boxes, dtype=torch.float32)\n                    targets[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n\n        # --- Augmentation ---\n        if self.augment:\n            if random.random() > 0.5:  # horizontal flip\n                image = cv2.flip(image, 1)\n                if targets[\"boxes\"].numel() > 0:\n                    targets[\"boxes\"][:, 0] = w - targets[\"boxes\"][:, 0]  # flip x center\n\n        # --- Final conversion ---\n        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n        \n        return image, targets\ndef create_fire_datasets(dataset_base_path, img_size=640):\n    train_dataset = FireDetectionDataset(\n        os.path.join(dataset_base_path, 'train', 'images'),\n        os.path.join(dataset_base_path, 'train', 'labels'),\n        img_size=img_size, augment=True\n    )\n    \n    val_dataset = FireDetectionDataset(\n        os.path.join(dataset_base_path, 'val', 'images'),\n        os.path.join(dataset_base_path, 'val', 'labels'),\n        img_size=img_size, augment=False\n    )\n    \n    test_dataset = FireDetectionDataset(\n        os.path.join(dataset_base_path, 'test', 'images'),\n        os.path.join(dataset_base_path, 'test', 'labels'),\n        img_size=img_size, augment=False\n    )\n    \n    return train_dataset, val_dataset, test_dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T00:39:21.664492Z","iopub.execute_input":"2025-09-10T00:39:21.665175Z","iopub.status.idle":"2025-09-10T00:39:21.676356Z","shell.execute_reply.started":"2025-09-10T00:39:21.665152Z","shell.execute_reply":"2025-09-10T00:39:21.675642Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from types import SimpleNamespace\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Model\ndataset_path = \"/kaggle/input/home-fire-dataset\"\nweights_path = download_convnext_weights(\"convnext_tiny_1k\")\nbatch_size=8    \n    # Create datasets\nprint(\"Creating datasets...\")\ntrain_dataset, val_dataset, test_dataset = create_fire_datasets(dataset_path)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n                          num_workers=2, pin_memory=True, drop_last=True, collate_fn=lambda b: tuple(zip(*b)))\n\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n                        num_workers=2, pin_memory=True, drop_last=False, collate_fn=lambda b: tuple(zip(*b)))\n\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n                         num_workers=2, pin_memory=True, drop_last=False, collate_fn=lambda b: tuple(zip(*b)))\n    \nprint(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n    \n    # Create model\nprint(\"Creating WorkingConvNextYOLO model...\")\nmodel = WorkingConvNextYOLO(\n        yolo_path=\"yolov8n.pt\",\n        num_classes=2,  # fire and smoke\n        convnext_weights_path=weights_path,\n        freeze_backbone=True\n    )\nmodel = model.to(device)\n# Loss\ncriterion = YOLOLoss(num_classes=2)\n\n# Optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n# Training loop (simplified)\nnum_epochs = 10  # set how many epochs you want\n\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0.0\n    epoch_lbox = 0.0\n    epoch_lobj = 0.0\n    epoch_lcls = 0.0\n    num_batches = 0\n    for images, targets in train_loader:  # images: [B,3,H,W], targets: list of dicts\n        images = torch.stack(images).to(device) \n        \n        preds = model(images)   # This is YOLO forward\n        # ❗ Ensure preds are reshaped as your loss expects\n        # Example: preds already come as list of feature maps [B,C,H,W]\n        # Convert to [B,H,W,C]\n        if isinstance(preds, (list, tuple)):\n            preds = preds[0]   # ✅ now preds is [B,6,N]\n        num_classes = 2\n        reg_max = 16\n        no = num_classes + 4 * reg_max\n        B, C, H, W = preds.shape\n        na = C // no  \n        \n        # reshape to [B, na, no, H, W]\n        preds = preds.view(B, na, no, H, W)\n        \n        # move dims -> [B, H, W, na, no]\n        preds = preds.permute(0, 3, 4, 1, 2)\n        \n        # flatten -> [B, N, no]\n        preds = preds.reshape(B, -1, no)\n        loss, loss_items = criterion(preds, targets)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        print(loss_items)\n        epoch_loss += loss.item()\n        epoch_lbox += loss_items[\"lbox\"].item()\n        epoch_lobj += loss_items[\"lobj\"].item()\n        epoch_lcls += loss_items[\"lcls\"].item()\n        num_batches += 1\n\n    # Average over batches\n    avg_loss = epoch_loss / num_batches\n    avg_lbox = epoch_lbox / num_batches\n    avg_lobj = epoch_lobj / num_batches\n    avg_lcls = epoch_lcls / num_batches\n    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n          f\"Loss: {avg_loss:.4f}, \"\n          f\"lbox: {avg_lbox:.4f}, \"\n          f\"lobj: {avg_lobj:.4f}, \"\n          f\"lcls: {avg_lcls:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T00:40:04.599836Z","iopub.execute_input":"2025-09-10T00:40:04.600138Z","iopub.status.idle":"2025-09-10T00:40:14.908207Z","shell.execute_reply.started":"2025-09-10T00:40:04.600119Z","shell.execute_reply":"2025-09-10T00:40:14.907270Z"}},"outputs":[{"name":"stdout","text":"✅ Found existing weights: convnext_tiny_1k.pth\nCreating datasets...\nFound 3900 images in /kaggle/input/home-fire-dataset/train/images\nFound 1300 images in /kaggle/input/home-fire-dataset/val/images\nFound 1300 images in /kaggle/input/home-fire-dataset/test/images\nTrain batches: 487, Val batches: 163\nCreating WorkingConvNextYOLO model...\nCreating Working ConvNext-YOLO...\nOriginal YOLO type: <class 'ultralytics.nn.tasks.DetectionModel'>\nModel layers type: <class 'torch.nn.modules.container.Sequential'>\nNumber of layers: 23\nReplacing backbone with ConvNeXt...\n🔄 Loading pretrained weights from convnext_tiny_1k.pth\n✅ Pretrained weights loaded successfully!\n✅ ConvNext Tiny backbone ready for YOLO integration\n📊 Feature channels: [192, 384, 768] → [256, 512, 1024]\nswapped\nFreezing ConvNeXt backbone...\nUpdating detection head: -> 2 classes\nFound detection layer: <class 'ultralytics.nn.modules.head.Detect'>\n  Updated cv2[0]: 64 -> 2\n  Updated cv2[1]: 64 -> 2\n  Updated cv2[2]: 64 -> 2\n  Updated cv3[0]: 80 -> 64\n  Updated cv3[1]: 80 -> 64\n  Updated cv3[2]: 80 -> 64\nEnsuring neck and head are trainable...\n\nParameter Summary:\n  Total: 31,995,590 (32.0M)\n  Trainable: 3,141,222 (3.1M)\n  Frozen: 28,854,368 (28.9M)\n  Trainable Ratio: 9.8%\n\nComponent Breakdown:\n  Backbone: 28,854,368 total, 0 trainable\n  Detection Head: 881,686 total, 881,686 trainable\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(7.8244, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(6.4600, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(5.8733, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(5.5346, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(4.4147, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(4.3150, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(4.1278, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(4.0366, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(3.7430, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(3.5636, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(3.5712, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(3.3341, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(3.3106, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(3.0971, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(2.9748, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(2.8379, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(2.7587, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(2.6186, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(2.5041, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(2.4060, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(2.2688, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(2.1721, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(2.0142, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(1.9139, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(1.8001, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(1.7542, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(1.6045, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(1.5311, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(1.4312, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(1.3912, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n{'lbox': tensor(0., device='cuda:0'), 'lobj': tensor(1.2340, device='cuda:0'), 'lcls': tensor(0., device='cuda:0')}\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1939771249.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# images: [B,3,H,W], targets: list of dicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# This is YOLO forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"\nfrom __future__ import annotations\n\nfrom copy import copy\nfrom typing import Any\n\nimport torch\n\nfrom ultralytics.data import ClassificationDataset, build_dataloader\nfrom ultralytics.engine.trainer import BaseTrainer\nfrom ultralytics.models import yolo\nfrom ultralytics.nn.tasks import ClassificationModel\nfrom ultralytics.utils import DEFAULT_CFG, LOGGER, RANK\nfrom ultralytics.utils.plotting import plot_images, plot_results\nfrom ultralytics.utils.torch_utils import is_parallel, strip_optimizer, torch_distributed_zero_first\n\n\nclass ClassificationTrainer(BaseTrainer):\n    \"\"\"\n    A trainer class extending BaseTrainer for training image classification models.\n\n    This trainer handles the training process for image classification tasks, supporting both YOLO classification models\n    and torchvision models with comprehensive dataset handling and validation.\n\n    Attributes:\n        model (ClassificationModel): The classification model to be trained.\n        data (dict[str, Any]): Dictionary containing dataset information including class names and number of classes.\n        loss_names (list[str]): Names of the loss functions used during training.\n        validator (ClassificationValidator): Validator instance for model evaluation.\n\n    Methods:\n        set_model_attributes: Set the model's class names from the loaded dataset.\n        get_model: Return a modified PyTorch model configured for training.\n        setup_model: Load, create or download model for classification.\n        build_dataset: Create a ClassificationDataset instance.\n        get_dataloader: Return PyTorch DataLoader with transforms for image preprocessing.\n        preprocess_batch: Preprocess a batch of images and classes.\n        progress_string: Return a formatted string showing training progress.\n        get_validator: Return an instance of ClassificationValidator.\n        label_loss_items: Return a loss dict with labelled training loss items.\n        plot_metrics: Plot metrics from a CSV file.\n        final_eval: Evaluate trained model and save validation results.\n        plot_training_samples: Plot training samples with their annotations.\n\n    Examples:\n        Initialize and train a classification model\n        >>> from ultralytics.models.yolo.classify import ClassificationTrainer\n        >>> args = dict(model=\"yolo11n-cls.pt\", data=\"imagenet10\", epochs=3)\n        >>> trainer = ClassificationTrainer(overrides=args)\n        >>> trainer.train()\n    \"\"\"\n\n    def __init__(self, cfg=DEFAULT_CFG, overrides: dict[str, Any] | None = None, _callbacks=None):\n        \"\"\"\n        Initialize a ClassificationTrainer object.\n\n        This constructor sets up a trainer for image classification tasks, configuring the task type and default\n        image size if not specified.\n\n        Args:\n            cfg (dict[str, Any], optional): Default configuration dictionary containing training parameters.\n            overrides (dict[str, Any], optional): Dictionary of parameter overrides for the default configuration.\n            _callbacks (list[Any], optional): List of callback functions to be executed during training.\n\n        Examples:\n            Create a trainer with custom configuration\n            >>> from ultralytics.models.yolo.classify import ClassificationTrainer\n            >>> args = dict(model=\"yolo11n-cls.pt\", data=\"imagenet10\", epochs=3)\n            >>> trainer = ClassificationTrainer(overrides=args)\n            >>> trainer.train()\n        \"\"\"\n        if overrides is None:\n            overrides = {}\n        overrides[\"task\"] = \"classify\"\n        if overrides.get(\"imgsz\") is None:\n            overrides[\"imgsz\"] = 224\n        super().__init__(cfg, overrides, _callbacks)\n\n    def set_model_attributes(self):\n        \"\"\"Set the YOLO model's class names from the loaded dataset.\"\"\"\n        self.model.names = self.data[\"names\"]\n\n    def get_model(self, cfg=None, weights=None, verbose: bool = True):\n        \"\"\"\n        Return a modified PyTorch model configured for training YOLO classification.\n\n        Args:\n            cfg (Any, optional): Model configuration.\n            weights (Any, optional): Pre-trained model weights.\n            verbose (bool, optional): Whether to display model information.\n\n        Returns:\n            (ClassificationModel): Configured PyTorch model for classification.\n        \"\"\"\n        model = ClassificationModel(cfg, nc=self.data[\"nc\"], ch=self.data[\"channels\"], verbose=verbose and RANK == -1)\n        if weights:\n            model.load(weights)\n\n        for m in model.modules():\n            if not self.args.pretrained and hasattr(m, \"reset_parameters\"):\n                m.reset_parameters()\n            if isinstance(m, torch.nn.Dropout) and self.args.dropout:\n                m.p = self.args.dropout  # set dropout\n        for p in model.parameters():\n            p.requires_grad = True  # for training\n        return model\n\n    def setup_model(self):\n        \"\"\"\n        Load, create or download model for classification tasks.\n\n        Returns:\n            (Any): Model checkpoint if applicable, otherwise None.\n        \"\"\"\n        import torchvision  # scope for faster 'import ultralytics'\n\n        if str(self.model) in torchvision.models.__dict__:\n            self.model = torchvision.models.__dict__[self.model](\n                weights=\"IMAGENET1K_V1\" if self.args.pretrained else None\n            )\n            ckpt = None\n        else:\n            ckpt = super().setup_model()\n        ClassificationModel.reshape_outputs(self.model, self.data[\"nc\"])\n        return ckpt\n\n    def build_dataset(self, img_path: str, mode: str = \"train\", batch=None):\n        \"\"\"\n        Create a ClassificationDataset instance given an image path and mode.\n\n        Args:\n            img_path (str): Path to the dataset images.\n            mode (str, optional): Dataset mode ('train', 'val', or 'test').\n            batch (Any, optional): Batch information (unused in this implementation).\n\n        Returns:\n            (ClassificationDataset): Dataset for the specified mode.\n        \"\"\"\n        return ClassificationDataset(root=img_path, args=self.args, augment=mode == \"train\", prefix=mode)\n\n    def get_dataloader(self, dataset_path: str, batch_size: int = 16, rank: int = 0, mode: str = \"train\"):\n        \"\"\"\n        Return PyTorch DataLoader with transforms to preprocess images.\n\n        Args:\n            dataset_path (str): Path to the dataset.\n            batch_size (int, optional): Number of images per batch.\n            rank (int, optional): Process rank for distributed training.\n            mode (str, optional): 'train', 'val', or 'test' mode.\n\n        Returns:\n            (torch.utils.data.DataLoader): DataLoader for the specified dataset and mode.\n        \"\"\"\n        with torch_distributed_zero_first(rank):  # init dataset *.cache only once if DDP\n            dataset = self.build_dataset(dataset_path, mode)\n\n        loader = build_dataloader(dataset, batch_size, self.args.workers, rank=rank)\n        # Attach inference transforms\n        if mode != \"train\":\n            if is_parallel(self.model):\n                self.model.module.transforms = loader.dataset.torch_transforms\n            else:\n                self.model.transforms = loader.dataset.torch_transforms\n        return loader\n\n    def preprocess_batch(self, batch: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n        \"\"\"Preprocess a batch of images and classes.\"\"\"\n        batch[\"img\"] = batch[\"img\"].to(self.device, non_blocking=True)\n        batch[\"cls\"] = batch[\"cls\"].to(self.device, non_blocking=True)\n        return batch\n\n    def progress_string(self) -> str:\n        \"\"\"Return a formatted string showing training progress.\"\"\"\n        return (\"\\n\" + \"%11s\" * (4 + len(self.loss_names))) % (\n            \"Epoch\",\n            \"GPU_mem\",\n            *self.loss_names,\n            \"Instances\",\n            \"Size\",\n        )\n\n    def get_validator(self):\n        \"\"\"Return an instance of ClassificationValidator for validation.\"\"\"\n        self.loss_names = [\"loss\"]\n        return yolo.classify.ClassificationValidator(\n            self.test_loader, self.save_dir, args=copy(self.args), _callbacks=self.callbacks\n        )\n\n    def label_loss_items(self, loss_items: torch.Tensor | None = None, prefix: str = \"train\"):\n        \"\"\"\n        Return a loss dict with labelled training loss items tensor.\n\n        Args:\n            loss_items (torch.Tensor, optional): Loss tensor items.\n            prefix (str, optional): Prefix to prepend to loss names.\n\n        Returns:\n            keys (list[str]): List of loss keys if loss_items is None.\n            loss_dict (dict[str, float]): Dictionary of loss items if loss_items is provided.\n        \"\"\"\n        keys = [f\"{prefix}/{x}\" for x in self.loss_names]\n        if loss_items is None:\n            return keys\n        loss_items = [round(float(loss_items), 5)]\n        return dict(zip(keys, loss_items))\n\n    def plot_metrics(self):\n        \"\"\"Plot metrics from a CSV file.\"\"\"\n        plot_results(file=self.csv, classify=True, on_plot=self.on_plot)  # save results.png\n\n    def final_eval(self):\n        \"\"\"Evaluate trained model and save validation results.\"\"\"\n        for f in self.last, self.best:\n            if f.exists():\n                strip_optimizer(f)  # strip optimizers\n                if f is self.best:\n                    LOGGER.info(f\"\\nValidating {f}...\")\n                    self.validator.args.data = self.args.data\n                    self.validator.args.plots = self.args.plots\n                    self.metrics = self.validator(model=f)\n                    self.metrics.pop(\"fitness\", None)\n                    self.run_callbacks(\"on_fit_epoch_end\")\n\n    def plot_training_samples(self, batch: dict[str, torch.Tensor], ni: int):\n        \"\"\"\n        Plot training samples with their annotations.\n\n        Args:\n            batch (dict[str, torch.Tensor]): Batch containing images and class labels.\n            ni (int): Number of iterations.\n        \"\"\"\n        batch[\"batch_idx\"] = torch.arange(len(batch[\"img\"]))  # add batch index for plotting\n        plot_images(\n            labels=batch,\n            fname=self.save_dir / f\"train_batch{ni}.jpg\",\n            on_plot=self.on_plot,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T00:40:18.217311Z","iopub.execute_input":"2025-09-10T00:40:18.217606Z","iopub.status.idle":"2025-09-10T00:40:18.237470Z","shell.execute_reply.started":"2025-09-10T00:40:18.217582Z","shell.execute_reply":"2025-09-10T00:40:18.236807Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from ultralytics.models.yolo.detect.train import DetectionTrainer\nfrom types import SimpleNamespace\nfrom ultralytics.utils.loss import v8DetectionLoss\n\nclass ConvNextYOLOTrainer(DetectionTrainer):\n    def get_model(self, cfg=None, weights=None, verbose=True):\n        \"\"\"\n        Override YOLO model loading with our custom ConvNeXt backbone.\n        \"\"\"\n        # Initialize your custom YOLO model\n        weights_path = download_convnext_weights(\"convnext_tiny_1k\")\n        model = WorkingConvNextYOLO(\n            yolo_path=\"yolov8n.pt\",\n            num_classes=self.data[\"nc\"],     # number of dataset classes\n            convnext_weights_path = weights_path,      # or your convnext pretrained weights\n            freeze_backbone=True             # freeze backbone if needed\n        ).yolo\n\n        \n        keys = [\"box\", \"cls\", \"dfl\", \"pose\", \"kobj\"]\n        hyp_dict = {k: getattr(self.args, k) for k in keys if hasattr(self.args, k)}\n        print(\"⚠️ Building hyp from args:\", hyp_dict)\n        self.hyp = SimpleNamespace(**hyp_dict)\n            \n        print(\"done#########################\")\n        # Load weights if provided\n        \n        print(\"done2#########################################\")\n         # Build SimpleNamespace for hyp\n        keys = [\"box\", \"cls\", \"dfl\", \"pose\", \"kobj\"]\n        hyp_dict = {k: getattr(self.args, k) for k in keys if hasattr(self.args, k)}\n        hyp_ns = SimpleNamespace(**hyp_dict)\n\n        # Attach hyp to the model (NOT to args)\n        model.model.args = hyp_ns\n        print(model.args)\n        # Initialize loss so the trainer can call it\n        model.init_criterion()  # ensures self.criterion exists\n        \n        return model\nimport torch, gc\ngc.collect()\ntorch.cuda.empty_cache()\nargs = dict(\n    model=\"yolov8n.pt\",   # dummy, will be replaced by get_model()\n    data=\"/kaggle/working/home-fire.yaml\",  # your dataset yaml\n    epochs=50,\n    imgsz=640,\n    batch=8\n)\n\ntrainer = ConvNextYOLOTrainer(overrides=args)\nprint(\"trainer is ready\")\n\n# Build hyp from args manually\n# Do NOT assign to trainer.args.hyp\n# trainer.args.hyp = trainer.hyp  <-- remove this line\n\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T00:42:35.382239Z","iopub.execute_input":"2025-09-10T00:42:35.382602Z"}},"outputs":[{"name":"stdout","text":"Ultralytics 8.3.197 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/kaggle/working/home-fire.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/kaggle/working/runs/detect/train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\ntrainer is ready\n✅ Found existing weights: convnext_tiny_1k.pth\nCreating Working ConvNext-YOLO...\nOriginal YOLO type: <class 'ultralytics.nn.tasks.DetectionModel'>\nModel layers type: <class 'torch.nn.modules.container.Sequential'>\nNumber of layers: 23\nReplacing backbone with ConvNeXt...\n🔄 Loading pretrained weights from convnext_tiny_1k.pth\n✅ Pretrained weights loaded successfully!\n✅ ConvNext Tiny backbone ready for YOLO integration\n📊 Feature channels: [192, 384, 768] → [256, 512, 1024]\nswapped\nFreezing ConvNeXt backbone...\nUpdating detection head: -> 2 classes\nFound detection layer: <class 'ultralytics.nn.modules.head.Detect'>\n  Updated cv2[0]: 64 -> 2\n  Updated cv2[1]: 64 -> 2\n  Updated cv2[2]: 64 -> 2\n  Updated cv3[0]: 80 -> 64\n  Updated cv3[1]: 80 -> 64\n  Updated cv3[2]: 80 -> 64\nEnsuring neck and head are trainable...\n\nParameter Summary:\n  Total: 31,995,590 (32.0M)\n  Trainable: 3,141,222 (3.1M)\n  Frozen: 28,854,368 (28.9M)\n  Trainable Ratio: 9.8%\n\nComponent Breakdown:\n  Backbone: 28,854,368 total, 0 trainable\n  Detection Head: 881,686 total, 881,686 trainable\n⚠️ Building hyp from args: {'box': 7.5, 'cls': 0.5, 'dfl': 1.5, 'pose': 12.0, 'kobj': 1.0}\ndone#########################\ndone2#########################################\n{'task': 'detect', 'data': 'coco.yaml', 'imgsz': 640, 'single_cls': False, 'model': 'yolov8n.pt'}\nFreezing layer 'model.22.dfl.conv.weight'\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.downsample_layers.0.0.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.downsample_layers.0.0.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.downsample_layers.0.1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.downsample_layers.0.1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.downsample_layers.1.0.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.downsample_layers.1.0.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.downsample_layers.1.1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.downsample_layers.1.1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.downsample_layers.2.0.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.downsample_layers.2.0.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.downsample_layers.2.1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.downsample_layers.2.1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.downsample_layers.3.0.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.downsample_layers.3.0.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.downsample_layers.3.1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.downsample_layers.3.1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.0.gamma'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.0.dwconv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.0.dwconv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.0.norm.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.0.norm.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.0.pwconv1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.0.pwconv1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.0.pwconv2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.0.pwconv2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.1.gamma'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.1.dwconv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.1.dwconv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.1.norm.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.1.norm.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.1.pwconv1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.1.pwconv1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.1.pwconv2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.1.pwconv2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.2.gamma'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.2.dwconv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.2.dwconv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.2.norm.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.2.norm.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.2.pwconv1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.2.pwconv1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.2.pwconv2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.0.2.pwconv2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.0.gamma'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.0.dwconv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.0.dwconv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.0.norm.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.0.norm.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.0.pwconv1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.0.pwconv1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.0.pwconv2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.0.pwconv2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.1.gamma'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.1.dwconv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.1.dwconv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.1.norm.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.1.norm.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.1.pwconv1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.1.pwconv1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.1.pwconv2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.1.pwconv2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.2.gamma'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.2.dwconv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.2.dwconv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.2.norm.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.2.norm.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.2.pwconv1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.2.pwconv1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.2.pwconv2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.1.2.pwconv2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.0.gamma'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.0.dwconv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.0.dwconv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.0.norm.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.0.norm.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.0.pwconv1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.0.pwconv1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.0.pwconv2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.0.pwconv2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.1.gamma'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.1.dwconv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.1.dwconv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.1.norm.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.1.norm.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.1.pwconv1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.1.pwconv1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.1.pwconv2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.1.pwconv2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.2.gamma'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.2.dwconv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.2.dwconv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.2.norm.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.2.norm.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.2.pwconv1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.2.pwconv1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.2.pwconv2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.2.pwconv2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.3.gamma'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.3.dwconv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.3.dwconv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.3.norm.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.3.norm.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.3.pwconv1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.3.pwconv1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.3.pwconv2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.3.pwconv2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.4.gamma'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.4.dwconv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.4.dwconv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.4.norm.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.4.norm.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.4.pwconv1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.4.pwconv1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.4.pwconv2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.4.pwconv2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.5.gamma'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.5.dwconv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.5.dwconv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.5.norm.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.5.norm.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.5.pwconv1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.5.pwconv1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.5.pwconv2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.5.pwconv2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.6.gamma'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.6.dwconv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.6.dwconv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.6.norm.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.6.norm.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.6.pwconv1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.6.pwconv1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.6.pwconv2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.6.pwconv2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.7.gamma'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.7.dwconv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.7.dwconv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.7.norm.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.7.norm.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.7.pwconv1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.7.pwconv1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.7.pwconv2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.7.pwconv2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.8.gamma'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.8.dwconv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.8.dwconv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.8.norm.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.8.norm.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.8.pwconv1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.8.pwconv1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.8.pwconv2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.2.8.pwconv2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.0.gamma'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.0.dwconv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.0.dwconv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.0.norm.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.0.norm.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.0.pwconv1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.0.pwconv1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.0.pwconv2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.0.pwconv2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.1.gamma'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.1.dwconv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.1.dwconv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.1.norm.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.1.norm.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.1.pwconv1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.1.pwconv1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.1.pwconv2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.1.pwconv2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.2.gamma'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.2.dwconv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.2.dwconv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.2.norm.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.2.norm.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.2.pwconv1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.2.pwconv1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.2.pwconv2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.convnext.stages.3.2.pwconv2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.adapters.0.0.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.adapters.0.1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.adapters.0.1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.adapters.1.0.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.adapters.1.1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.adapters.1.1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.adapters.2.0.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.adapters.2.1.weight'. See ultralytics.engine.trainer for customization of frozen layers.\nWARNING ⚠️ setting 'requires_grad=True' for frozen layer 'backbone.adapters.2.1.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% ━━━━━━━━━━━━ 5.4MB 14.5MB/s 0.4s.3s<0.2s.0s\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 39.6±19.3 MB/s, size: 289.5 KB)\n\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/home-fire-dataset/train/labels... 3900 images, 87 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 3900/3900 381.3it/s 10.2s<0.0s\nWARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/home-fire-dataset/train is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 59.7±22.3 MB/s, size: 306.0 KB)\n\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/home-fire-dataset/val/labels... 1300 images, 33 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1300/1300 339.2it/s 3.8s0.1s\nWARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/home-fire-dataset/val is not writeable, cache not saved.\nPlotting labels to /kaggle/working/runs/detect/train2/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 60 weight(decay=0.0), 165 weight(decay=0.0005), 146 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1m/kaggle/working/runs/detect/train2\u001b[0m\nStarting training for 50 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K       1/50      1.52G      3.143      92.56      3.697          7        640: 100% ━━━━━━━━━━━━ 488/488 7.7it/s 1:04<0.1ss\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 82/82 9.7it/s 8.4s0.2ss\n                   all       1300       1580      0.226      0.152      0.146      0.059\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K       2/50      2.33G      2.143      2.329      2.179         12        640: 100% ━━━━━━━━━━━━ 488/488 8.3it/s 59.0s<0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 82/82 11.8it/s 7.0s0.1s\n                   all       1300       1580      0.485      0.376      0.402      0.196\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K       3/50      2.35G      1.944       2.01      1.977          5        640: 100% ━━━━━━━━━━━━ 488/488 8.4it/s 57.8s<0.1s\n\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 82/82 11.7it/s 7.0s0.1s\n                   all       1300       1580      0.706      0.394      0.475      0.233\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n\u001b[K       4/50      2.35G      1.856       1.84      1.871         14        640: 87% ━━━━━━━━━━── 426/488 8.3it/s 50.5s<7.5ss","output_type":"stream"}],"execution_count":null}]}