{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T20:09:03.297801Z",
     "iopub.status.busy": "2025-09-11T20:09:03.297608Z",
     "iopub.status.idle": "2025-09-11T20:09:22.929349Z",
     "shell.execute_reply": "2025-09-11T20:09:22.928590Z",
     "shell.execute_reply.started": "2025-09-11T20:09:03.297784Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers@main\n",
      "  Cloning https://github.com/huggingface/transformers (to revision main) to /tmp/pip-req-build-xc2w9pbl\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-xc2w9pbl\n",
      "  Resolved https://github.com/huggingface/transformers to commit cf084f5b40e19b5a5f946cee75bead6d4247b071\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.57.0.dev0) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.57.0.dev0) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.57.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.57.0.dev0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.57.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.57.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.57.0.dev0) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.57.0.dev0) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.57.0.dev0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.57.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.0.dev0) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.0.dev0) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.0.dev0) (1.1.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.57.0.dev0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.57.0.dev0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.57.0.dev0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.57.0.dev0) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.57.0.dev0) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.57.0.dev0) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.57.0.dev0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.57.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.57.0.dev0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.57.0.dev0) (2025.6.15)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.57.0.dev0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.57.0.dev0) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.57.0.dev0) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.57.0.dev0) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.57.0.dev0) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install git+https://github.com/huggingface/transformers@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T20:09:22.930394Z",
     "iopub.status.busy": "2025-09-11T20:09:22.930183Z",
     "iopub.status.idle": "2025-09-11T20:09:27.804135Z",
     "shell.execute_reply": "2025-09-11T20:09:27.803442Z",
     "shell.execute_reply.started": "2025-09-11T20:09:22.930374Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57.0.dev0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T20:09:47.981667Z",
     "iopub.status.busy": "2025-09-11T20:09:47.981143Z",
     "iopub.status.idle": "2025-09-11T20:09:47.992981Z",
     "shell.execute_reply": "2025-09-11T20:09:47.992055Z",
     "shell.execute_reply.started": "2025-09-11T20:09:47.981643Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Basic building blocks\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=None, groups=1):\n",
    "        super().__init__()\n",
    "        if padding is None:\n",
    "            padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, shortcut=True, expansion=0.5):\n",
    "        super().__init__()\n",
    "        hidden_channels = int(out_channels * expansion)\n",
    "        self.conv1 = Conv(in_channels, hidden_channels, 1, 1)\n",
    "        self.conv2 = Conv(hidden_channels, out_channels, 3, 1)\n",
    "        self.add = shortcut and in_channels == out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv2(self.conv1(x)) if self.add else self.conv2(self.conv1(x))\n",
    "\n",
    "class C3k2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n=1, shortcut=True, expansion=0.5):\n",
    "        super().__init__()\n",
    "        mid_channels = out_channels // 2\n",
    "        self.conv1 = Conv(in_channels, mid_channels, 1, 1)\n",
    "        self.conv2 = Conv(in_channels, mid_channels, 1, 1)\n",
    "        self.res_blocks = nn.Sequential(*[Bottleneck(mid_channels, mid_channels, shortcut, expansion) for _ in range(n)])\n",
    "        self.conv3 = Conv(2 * mid_channels, out_channels, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.res_blocks(self.conv1(x))\n",
    "        y2 = self.conv2(x)\n",
    "        return self.conv3(torch.cat((y1, y2), dim=1))\n",
    "\n",
    "class Detect(nn.Module):\n",
    "    def __init__(self, nc=2, ch=(256, 256, 256), stride=(8, 16, 32), reg_max=0):\n",
    "        super().__init__()\n",
    "        self.nc = nc\n",
    "        self.nl = len(ch)\n",
    "        self.no = (reg_max * 4 if reg_max > 0 else 4) + 1 + nc\n",
    "        self.stride = stride\n",
    "        self.reg_max = reg_max\n",
    "\n",
    "        self.cv2 = nn.ModuleList()\n",
    "        self.cv3 = nn.ModuleList()\n",
    "        for in_channels in ch:\n",
    "            self.cv2.append(Conv(in_channels, in_channels, 3, 1))\n",
    "            self.cv3.append(nn.Conv2d(in_channels, self.no, 1, 1))\n",
    "    def forward(self, x):\n",
    "        z = []\n",
    "        for i in range(self.nl):\n",
    "            xi = self.cv2[i](x[i])   # [B, C, H, W]\n",
    "            xi = self.cv3[i](xi)     # [B, no, H, W]\n",
    "            xi = xi.permute(0, 2, 3, 1).contiguous()  # [B, H, W, no]\n",
    "            z.append(xi)\n",
    "        return z\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T20:09:51.070570Z",
     "iopub.status.busy": "2025-09-11T20:09:51.069899Z",
     "iopub.status.idle": "2025-09-11T20:10:09.573717Z",
     "shell.execute_reply": "2025-09-11T20:10:09.573109Z",
     "shell.execute_reply.started": "2025-09-11T20:09:51.070523Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-11 20:09:56.870277: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757621397.046356     117 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757621397.100453     117 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import DINOv3ConvNextModel\n",
    "\n",
    "class DINOBackbone(nn.Module):\n",
    "    def __init__(self, model_name=\"facebook/dinov3-convnext-base-pretrain-lvd1689m\"):\n",
    "        super().__init__()\n",
    "        self.model = DINOv3ConvNextModel.from_pretrained(\n",
    "            model_name, \n",
    "            output_hidden_states=True  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pixel_values = x[\"pixel_values\"] \n",
    "\n",
    "        outputs = self.model(pixel_values)  \n",
    "        hidden_states = outputs.hidden_states \n",
    "        features = [hidden_states[i] for i in [2, 3, 4]]  \n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "class YoloDetectionHeadPath1(nn.Module):\n",
    "    def __init__(self, nc=2, backbone_channels=[384, 768, 1024], neck_out_dim=256):\n",
    "        super().__init__()\n",
    "        C3_ch, C4_ch, C5_ch = backbone_channels  # Backbone output channels: 384, 768, 1024\n",
    "        \n",
    "        # Channel reduction to standardize dimensions\n",
    "        self.reduce_p3 = Conv(C3_ch, neck_out_dim, 1, 1)  # 384 -> 256\n",
    "        self.reduce_p4 = Conv(C4_ch, neck_out_dim, 1, 1)  # 768 -> 256\n",
    "        self.reduce_p5 = Conv(C5_ch, neck_out_dim, 1, 1)  # 1024 -> 256\n",
    "        \n",
    "        # --- Upward Path ---\n",
    "        self.up_p5 = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        self.c3_p4 = C3k2(neck_out_dim * 2, neck_out_dim, shortcut=False)  # 512 -> 256\n",
    "\n",
    "        self.up_p4 = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        self.c3_p3 = C3k2(neck_out_dim * 2, neck_out_dim, shortcut=False)  # 512 -> 256\n",
    "\n",
    "        # --- Downward Path ---\n",
    "        self.down_p3 = Conv(neck_out_dim, neck_out_dim, 3, 2)  # 256 -> 256\n",
    "        self.c3_p4d = C3k2(neck_out_dim * 2, neck_out_dim, shortcut=False)  # 512 -> 256\n",
    "\n",
    "        self.down_p4 = Conv(neck_out_dim, neck_out_dim, 3, 2)  # 256 -> 256\n",
    "        self.c3_p5d = C3k2(neck_out_dim * 2, neck_out_dim, shortcut=True)  # 512 -> 256\n",
    "\n",
    "        # --- Detect Layer ---\n",
    "        self.detect = Detect(nc=nc, ch=[neck_out_dim, neck_out_dim, neck_out_dim])\n",
    "\n",
    "    def forward(self, p3, p4, p5):\n",
    "\n",
    "        p3 = self.reduce_p3(p3)\n",
    "        p4 = self.reduce_p4(p4)\n",
    "        p5 = self.reduce_p5(p5)\n",
    "        \n",
    "        # --- Upward path ---\n",
    "        p5_up = self.up_p5(p5)\n",
    "        p4_cat = torch.cat([p5_up, p4], dim=1)\n",
    "        p4_out = self.c3_p4(p4_cat)\n",
    "\n",
    "        p4_up = self.up_p4(p4_out)\n",
    "        p3_cat = torch.cat([p4_up, p3], dim=1)\n",
    "        p3_out = self.c3_p3(p3_cat)\n",
    "\n",
    "        # --- Downward path ---\n",
    "        p3_down = self.down_p3(p3_out)\n",
    "        p4_cat2 = torch.cat([p3_down, p4_out], dim=1)\n",
    "        p4_out2 = self.c3_p4d(p4_cat2)\n",
    "\n",
    "        p4_down = self.down_p4(p4_out2)\n",
    "        p5_cat2 = torch.cat([p4_down, p5], dim=1)\n",
    "        p5_out2 = self.c3_p5d(p5_cat2)\n",
    "\n",
    "        return self.detect([p3_out, p4_out2, p5_out2])\n",
    "\n",
    "\n",
    "\n",
    "class Path1Model(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.backbone = DINOBackbone()\n",
    "        self.head = YoloDetectionHeadPath1(\n",
    "            nc=num_classes,\n",
    "            backbone_channels=[256, 512, 1024],\n",
    "            neck_out_dim=256\n",
    "        )\n",
    "        self.num_classes = num_classes\n",
    "        self.args = {\n",
    "            \"box\": 7.5,   \n",
    "            \"cls\": 0.5,\n",
    "            \"dfl\": 1.5,\n",
    "        }\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        backbone_features = self.backbone(x)\n",
    "        p3, p4, p5 = backbone_features\n",
    "        \n",
    "        detect_output = self.head(p3, p4, p5)  \n",
    "        return detect_output  # tuple: (bbox_preds, class_logits)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T20:10:52.603759Z",
     "iopub.status.busy": "2025-09-11T20:10:52.603118Z",
     "iopub.status.idle": "2025-09-11T20:10:52.969960Z",
     "shell.execute_reply": "2025-09-11T20:10:52.969363Z",
     "shell.execute_reply.started": "2025-09-11T20:10:52.603734Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded to: /kaggle/input/home-fire-dataset\n",
      "Files inside: ['val', 'test', 'train']\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "\n",
    "# 🔹 Download the dataset\n",
    "path = kagglehub.dataset_download(\"pengbo00/home-fire-dataset\")\n",
    "\n",
    "print(\"Dataset downloaded to:\", path)\n",
    "print(\"Files inside:\", os.listdir(path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T20:10:56.391896Z",
     "iopub.status.busy": "2025-09-11T20:10:56.391627Z",
     "iopub.status.idle": "2025-09-11T20:10:56.399949Z",
     "shell.execute_reply": "2025-09-11T20:10:56.399228Z",
     "shell.execute_reply.started": "2025-09-11T20:10:56.391878Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class FireDataset(Dataset):\n",
    "    def __init__(self, root_dir, processor ,img_size = 640):\n",
    "        self.root_dir = root_dir\n",
    "        self.processor = processor\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.image_dir = os.path.join(root_dir, \"images\")\n",
    "        self.label_dir = os.path.join(root_dir, \"labels\")\n",
    "\n",
    "        self.image_files = sorted(os.listdir(self.image_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        img_file  = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir,img_file)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        inputs = self.processor(image)\n",
    "        pixel_values = inputs[\"pixel_values\"].squeeze(0) \n",
    "        label_file = os.path.splitext(img_file)[0] + \".txt\"\n",
    "        label_path = os.path.join(self.label_dir, label_file)\n",
    "\n",
    "        boxes = []\n",
    "        classes = []\n",
    "\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, \"r\") as f:\n",
    "                for line in f.readlines():\n",
    "                    cls, x, y, w, h = map(float, line.strip().split())\n",
    "                    classes.append(int(cls))\n",
    "                    boxes.append([x, y, w, h])\n",
    "\n",
    "        target = {\n",
    "            \"bboxes\": torch.tensor(boxes, dtype=torch.float32), \n",
    "            \"cls\": torch.tensor(classes, dtype=torch.long)    ,\n",
    "            \"gt_groups\": torch.tensor([len(boxes)], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "        return pixel_values, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T20:10:59.508901Z",
     "iopub.status.busy": "2025-09-11T20:10:59.508621Z",
     "iopub.status.idle": "2025-09-11T20:10:59.514664Z",
     "shell.execute_reply": "2025-09-11T20:10:59.513907Z",
     "shell.execute_reply.started": "2025-09-11T20:10:59.508882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "\n",
    "    batch_pixel_values = torch.stack(images)\n",
    "\n",
    "    bboxes = []\n",
    "    classes = []\n",
    "    gt_group_sizes = []  # number of boxes per image\n",
    "\n",
    "    for t in targets:\n",
    "        bboxes.append(t[\"bboxes\"])\n",
    "        classes.append(t[\"cls\"])\n",
    "        gt_group_sizes.append(len(t[\"cls\"]))\n",
    "\n",
    "    return batch_pixel_values, {\n",
    "        \"bboxes\": torch.cat(bboxes, dim=0) if bboxes else torch.empty((0, 4)),\n",
    "        \"cls\": torch.cat(classes, dim=0) if classes else torch.empty((0,), dtype=torch.long),\n",
    "        \"gt_groups\": torch.tensor(gt_group_sizes, dtype=torch.long)  # [bs] e.g. [3, 5]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T20:11:04.246947Z",
     "iopub.status.busy": "2025-09-11T20:11:04.246423Z",
     "iopub.status.idle": "2025-09-11T20:11:04.251791Z",
     "shell.execute_reply": "2025-09-11T20:11:04.250978Z",
     "shell.execute_reply.started": "2025-09-11T20:11:04.246923Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "class Processor:\n",
    "    def __init__(self, image_size=224):\n",
    "        # Base transform (like DINOv3 training)\n",
    "        self.preprocessor = AutoImageProcessor.from_pretrained(\"facebook/dinov3-convnext-base-pretrain-lvd1689m\")\n",
    "        self.extra_transforms = []\n",
    "\n",
    "    def add_transform(self, transform_fn):\n",
    "        self.extra_transforms.append(transform_fn)\n",
    "\n",
    "    def __call__(self, image):\n",
    "        for t in self.extra_transforms:\n",
    "            image = t(image)\n",
    "\n",
    "        return self.preprocessor(images=image, return_tensors=\"pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T20:11:06.768146Z",
     "iopub.status.busy": "2025-09-11T20:11:06.767869Z",
     "iopub.status.idle": "2025-09-11T20:11:07.243091Z",
     "shell.execute_reply": "2025-09-11T20:11:07.242471Z",
     "shell.execute_reply.started": "2025-09-11T20:11:06.768124Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a109a23c17324f61a13f13aff82ec7b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0aa5c813ff5423e9fb46420ecb00eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/585 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "processor = Processor()\n",
    "\n",
    "train_dataset = FireDataset(\"/kaggle/input/home-fire-dataset/train\", processor)\n",
    "val_dataset   = FireDataset(\"/kaggle/input/home-fire-dataset/val\", processor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T12:42:58.723236Z",
     "iopub.status.busy": "2025-09-11T12:42:58.722547Z",
     "iopub.status.idle": "2025-09-11T12:42:58.730970Z",
     "shell.execute_reply": "2025-09-11T12:42:58.729991Z",
     "shell.execute_reply.started": "2025-09-11T12:42:58.723191Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ContrastiveHeadWrapper(nn.Module):\n",
    "    def __init__(self, model, proj_dim=256):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        concat_dim = 256+ 512+ 1024\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(concat_dim, 512),  # P3+P4+P5 pooled concat\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, proj_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Backbone features\n",
    "        backbone_features = self.model.backbone(x)\n",
    "        # Head features (before detect)\n",
    "        p3, p4, p5 = backbone_features\n",
    "\n",
    "        # Global average pooling\n",
    "        pooled = []\n",
    "        for fm in [p3, p4, p5]:\n",
    "            pooled.append(F.adaptive_avg_pool2d(fm, (1, 1)).flatten(1))\n",
    "        pooled = torch.cat(pooled, dim=1)  # [B, 256*3]\n",
    "\n",
    "        return self.proj(pooled)  # [B, proj_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T05:47:49.276267Z",
     "iopub.status.busy": "2025-09-10T05:47:49.275983Z",
     "iopub.status.idle": "2025-09-10T05:47:49.281610Z",
     "shell.execute_reply": "2025-09-10T05:47:49.280784Z",
     "shell.execute_reply.started": "2025-09-10T05:47:49.276245Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.2):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, z1, z2):\n",
    "        z1 = F.normalize(z1, dim=-1)\n",
    "        z2 = F.normalize(z2, dim=-1)\n",
    "\n",
    "        logits = torch.mm(z1, z2.t()) / self.temperature\n",
    "        labels = torch.arange(z1.size(0), device=z1.device)\n",
    "        return F.cross_entropy(logits, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T12:03:11.210566Z",
     "iopub.status.busy": "2025-09-07T12:03:11.210289Z",
     "iopub.status.idle": "2025-09-07T12:48:15.027704Z",
     "shell.execute_reply": "2025-09-07T12:48:15.026930Z",
     "shell.execute_reply.started": "2025-09-07T12:03:11.210548Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.0949\n",
      "Epoch 2/20, Loss: 0.0526\n",
      "Epoch 3/20, Loss: 0.0491\n",
      "Epoch 4/20, Loss: 0.0469\n",
      "Epoch 5/20, Loss: 0.0440\n",
      "Epoch 6/20, Loss: 0.0422\n",
      "Epoch 7/20, Loss: 0.0391\n",
      "Epoch 8/20, Loss: 0.0398\n",
      "Epoch 9/20, Loss: 0.0396\n",
      "Epoch 10/20, Loss: 0.0421\n",
      "Epoch 11/20, Loss: 0.0374\n",
      "Epoch 12/20, Loss: 0.0388\n",
      "Epoch 13/20, Loss: 0.0372\n",
      "Epoch 14/20, Loss: 0.0376\n",
      "Epoch 15/20, Loss: 0.0366\n",
      "Epoch 16/20, Loss: 0.0389\n",
      "Epoch 17/20, Loss: 0.0328\n",
      "Epoch 18/20, Loss: 0.0365\n",
      "Epoch 19/20, Loss: 0.0346\n",
      "Epoch 20/20, Loss: 0.0350\n",
      "Projection head weights saved!\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "base_model = Path1Model(num_classes=2).to(device)\n",
    "\n",
    "# Freeze backbone if you only want to train head\n",
    "for param in base_model.backbone.parameters():\n",
    "    param.requires_grad = False  \n",
    "    \n",
    "contrastive_model = ContrastiveHeadWrapper(base_model, proj_dim=256).to(device)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(contrastive_model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "criterion = NTXentLoss(temperature=0.2)\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    contrastive_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        images, _ = batch  # FireDataset gives (image, target)\n",
    "        # Two random augmentations of the same batch\n",
    "        v1 = torch.stack([img for img in images]).to(device)\n",
    "        v2 = torch.stack([img for img in images]).to(device)  # here you should apply different augmentations!\n",
    "\n",
    "        z1 = contrastive_model({\"pixel_values\": v1})\n",
    "        z2 = contrastive_model({\"pixel_values\": v2})\n",
    "\n",
    "        loss = criterion(z1, z2)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "# Save only the projection head\n",
    "torch.save(contrastive_model.proj.state_dict(), \"contrastive_head.pth\")\n",
    "print(\"Projection head weights saved!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T20:11:19.107651Z",
     "iopub.status.busy": "2025-09-11T20:11:19.107339Z",
     "iopub.status.idle": "2025-09-11T20:11:19.117719Z",
     "shell.execute_reply": "2025-09-11T20:11:19.116957Z",
     "shell.execute_reply.started": "2025-09-11T20:11:19.107627Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DetectionLoss(nn.Module):\n",
    "    def __init__(self, num_classes, box_weight=10.0, obj_weight=1.0, cls_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.box_weight = box_weight\n",
    "        self.obj_weight = obj_weight\n",
    "        self.cls_weight = cls_weight\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        \"\"\"\n",
    "        preds: list of feature map heads [B, C, H, W] with C = 4 + 1 + nc\n",
    "        targets: list of dicts, length = B\n",
    "            - boxes: [N, 4] (xywh normalized)\n",
    "            - labels: [N] class indices\n",
    "        \"\"\"\n",
    "        device = preds[0].device\n",
    "        B = preds[0].size(0)\n",
    "        \n",
    "\n",
    "        flattened = []\n",
    "        for p in preds:  \n",
    "            B, H, W, no = p.shape\n",
    "            p = p.view(B, H * W, no)   \n",
    "            flattened.append(p)\n",
    "        \n",
    "        preds = torch.cat(flattened, dim=1)  \n",
    "             \n",
    "\n",
    "\n",
    "        pred_boxes = preds[..., :4]     \n",
    "        pred_obj   = preds[..., 4]      \n",
    "        pred_cls   = preds[..., 5:]       \n",
    "\n",
    "        num_preds = pred_boxes.size(1)\n",
    "\n",
    "        # init targets\n",
    "        obj_target = torch.zeros_like(pred_obj)           \n",
    "        cls_target = torch.zeros_like(pred_cls)           \n",
    "        box_target = torch.zeros_like(pred_boxes)          \n",
    "        box_mask   = torch.zeros_like(pred_obj, dtype=torch.bool)\n",
    "\n",
    "        # loop over batch\n",
    "        for b, t in enumerate(targets):\n",
    "            boxes = t[\"boxes\"].to(device)\n",
    "            labels = t[\"labels\"].to(device)\n",
    "\n",
    "            if boxes.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            # naive assignment: each GT → random prediction slot\n",
    "            for gt_box, gt_label in zip(boxes, labels):\n",
    "                idx = torch.randint(0, num_preds, (1,)).item()\n",
    "                box_target[b, idx] = gt_box\n",
    "                obj_target[b, idx] = 1.0\n",
    "                cls_target[b, idx, gt_label] = 1.0\n",
    "                box_mask[b, idx] = True\n",
    "\n",
    "        # losses\n",
    "        loss_box = (\n",
    "            F.smooth_l1_loss(pred_boxes[box_mask], box_target[box_mask])\n",
    "            if box_mask.any()\n",
    "            else torch.tensor(0., device=device)\n",
    "        )\n",
    "        loss_obj = F.binary_cross_entropy_with_logits(pred_obj, obj_target)\n",
    "        loss_cls = F.binary_cross_entropy_with_logits(pred_cls, cls_target)\n",
    "\n",
    "        total_loss = (\n",
    "            self.box_weight * loss_box +\n",
    "            self.obj_weight * loss_obj +\n",
    "            self.cls_weight * loss_cls\n",
    "        )\n",
    "\n",
    "        return total_loss, {\n",
    "            \"box_loss\": loss_box.item(),\n",
    "            \"obj_loss\": loss_obj.item(),\n",
    "            \"cls_loss\": loss_cls.item()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T20:12:38.313660Z",
     "iopub.status.busy": "2025-09-11T20:12:38.312848Z",
     "iopub.status.idle": "2025-09-11T22:30:50.600510Z",
     "shell.execute_reply": "2025-09-11T22:30:50.599585Z",
     "shell.execute_reply.started": "2025-09-11T20:12:38.313633Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79bb47f273d40a6b66af63817783e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/449 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71385f8ec50e4e7da58f53750470fbe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/350M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train Loss=0.578660 (box=0.0482, obj=0.0496, cls=0.0475) | Val Loss=0.404102 \n",
      " ✅ Saved new best model (val_loss=0.4041)\n",
      "Epoch 002 | Train Loss=0.370496 (box=0.0354, obj=0.0109, cls=0.0064) | Val Loss=0.201220 \n",
      " ✅ Saved new best model (val_loss=0.2012)\n",
      "Epoch 003 | Train Loss=0.300115 (box=0.0284, obj=0.0105, cls=0.0059) | Val Loss=0.841364 \n",
      "Epoch 004 | Train Loss=0.346310 (box=0.0330, obj=0.0105, cls=0.0058) | Val Loss=0.425857 \n",
      "Epoch 005 | Train Loss=0.343504 (box=0.0328, obj=0.0103, cls=0.0057) | Val Loss=0.350187 \n",
      "Epoch 006 | Train Loss=0.275559 (box=0.0261, obj=0.0098, cls=0.0055) | Val Loss=0.337532 \n",
      "Epoch 007 | Train Loss=0.247345 (box=0.0232, obj=0.0099, cls=0.0055) | Val Loss=0.295899 \n",
      "Epoch 008 | Train Loss=0.208416 (box=0.0194, obj=0.0097, cls=0.0053) | Val Loss=0.171640 \n",
      " ✅ Saved new best model (val_loss=0.1716)\n",
      "Epoch 009 | Train Loss=0.181610 (box=0.0167, obj=0.0095, cls=0.0052) | Val Loss=0.165997 \n",
      " ✅ Saved new best model (val_loss=0.1660)\n",
      "Epoch 010 | Train Loss=0.166843 (box=0.0153, obj=0.0094, cls=0.0051) | Val Loss=0.156596 \n",
      " ✅ Saved new best model (val_loss=0.1566)\n",
      "Epoch 011 | Train Loss=0.164947 (box=0.0151, obj=0.0093, cls=0.0051) | Val Loss=0.157955 \n",
      "Epoch 012 | Train Loss=0.163785 (box=0.0150, obj=0.0093, cls=0.0051) | Val Loss=0.148234 \n",
      " ✅ Saved new best model (val_loss=0.1482)\n",
      "Epoch 013 | Train Loss=0.160422 (box=0.0146, obj=0.0093, cls=0.0051) | Val Loss=0.140086 \n",
      " ✅ Saved new best model (val_loss=0.1401)\n",
      "Epoch 014 | Train Loss=0.160407 (box=0.0146, obj=0.0093, cls=0.0051) | Val Loss=0.142842 \n",
      "Epoch 015 | Train Loss=0.159954 (box=0.0146, obj=0.0093, cls=0.0050) | Val Loss=0.149257 \n",
      "Epoch 016 | Train Loss=0.156542 (box=0.0142, obj=0.0093, cls=0.0050) | Val Loss=0.147654 \n",
      "Epoch 017 | Train Loss=0.149774 (box=0.0136, obj=0.0093, cls=0.0050) | Val Loss=0.144961 \n",
      "Epoch 018 | Train Loss=0.151709 (box=0.0138, obj=0.0093, cls=0.0050) | Val Loss=0.135487 \n",
      " ✅ Saved new best model (val_loss=0.1355)\n",
      "Epoch 019 | Train Loss=0.149020 (box=0.0135, obj=0.0093, cls=0.0050) | Val Loss=0.128648 \n",
      " ✅ Saved new best model (val_loss=0.1286)\n",
      "Epoch 020 | Train Loss=0.147757 (box=0.0134, obj=0.0093, cls=0.0050) | Val Loss=0.131988 \n",
      "Epoch 021 | Train Loss=0.149632 (box=0.0135, obj=0.0093, cls=0.0050) | Val Loss=0.126477 \n",
      " ✅ Saved new best model (val_loss=0.1265)\n",
      "Epoch 022 | Train Loss=0.144833 (box=0.0131, obj=0.0093, cls=0.0050) | Val Loss=0.134747 \n",
      "Epoch 023 | Train Loss=0.145504 (box=0.0131, obj=0.0093, cls=0.0051) | Val Loss=0.124606 \n",
      " ✅ Saved new best model (val_loss=0.1246)\n",
      "Epoch 024 | Train Loss=0.138783 (box=0.0125, obj=0.0093, cls=0.0050) | Val Loss=0.144732 \n",
      "Epoch 025 | Train Loss=0.142263 (box=0.0128, obj=0.0093, cls=0.0050) | Val Loss=0.128624 \n",
      "Epoch 026 | Train Loss=0.139153 (box=0.0125, obj=0.0093, cls=0.0050) | Val Loss=0.124221 \n",
      " ✅ Saved new best model (val_loss=0.1242)\n",
      "Epoch 027 | Train Loss=0.135454 (box=0.0121, obj=0.0093, cls=0.0050) | Val Loss=0.129790 \n",
      "Epoch 028 | Train Loss=0.134647 (box=0.0120, obj=0.0093, cls=0.0050) | Val Loss=0.137818 \n",
      "Epoch 029 | Train Loss=0.134180 (box=0.0120, obj=0.0093, cls=0.0050) | Val Loss=0.115109 \n",
      " ✅ Saved new best model (val_loss=0.1151)\n",
      "Epoch 030 | Train Loss=0.130822 (box=0.0117, obj=0.0093, cls=0.0050) | Val Loss=0.134054 \n",
      "Epoch 031 | Train Loss=0.127939 (box=0.0114, obj=0.0093, cls=0.0050) | Val Loss=0.116952 \n",
      "Epoch 032 | Train Loss=0.128231 (box=0.0114, obj=0.0093, cls=0.0050) | Val Loss=0.127649 \n",
      "Epoch 033 | Train Loss=0.128676 (box=0.0115, obj=0.0093, cls=0.0050) | Val Loss=0.117955 \n",
      "Epoch 034 | Train Loss=0.126493 (box=0.0112, obj=0.0093, cls=0.0050) | Val Loss=0.113438 \n",
      " ✅ Saved new best model (val_loss=0.1134)\n",
      "Epoch 035 | Train Loss=0.123931 (box=0.0110, obj=0.0093, cls=0.0050) | Val Loss=0.131945 \n",
      "Epoch 036 | Train Loss=0.127013 (box=0.0113, obj=0.0093, cls=0.0050) | Val Loss=0.111529 \n",
      " ✅ Saved new best model (val_loss=0.1115)\n",
      "Epoch 037 | Train Loss=0.124784 (box=0.0111, obj=0.0093, cls=0.0050) | Val Loss=0.113121 \n",
      "Epoch 038 | Train Loss=0.122156 (box=0.0108, obj=0.0093, cls=0.0050) | Val Loss=0.130731 \n",
      "Epoch 039 | Train Loss=0.121670 (box=0.0107, obj=0.0093, cls=0.0050) | Val Loss=0.138389 \n",
      "Epoch 040 | Train Loss=0.122395 (box=0.0108, obj=0.0093, cls=0.0050) | Val Loss=0.111838 \n",
      "Epoch 041 | Train Loss=0.120544 (box=0.0106, obj=0.0093, cls=0.0050) | Val Loss=0.109172 \n",
      " ✅ Saved new best model (val_loss=0.1092)\n",
      "Epoch 042 | Train Loss=0.119063 (box=0.0105, obj=0.0093, cls=0.0050) | Val Loss=0.115724 \n",
      "Epoch 043 | Train Loss=0.116988 (box=0.0103, obj=0.0093, cls=0.0050) | Val Loss=0.112800 \n",
      "Epoch 044 | Train Loss=0.117487 (box=0.0103, obj=0.0093, cls=0.0050) | Val Loss=0.142736 \n",
      "Epoch 045 | Train Loss=0.118551 (box=0.0104, obj=0.0093, cls=0.0050) | Val Loss=0.105422 \n",
      " ✅ Saved new best model (val_loss=0.1054)\n",
      "Epoch 046 | Train Loss=0.115903 (box=0.0102, obj=0.0093, cls=0.0050) | Val Loss=0.116445 \n",
      "Epoch 047 | Train Loss=0.114097 (box=0.0100, obj=0.0093, cls=0.0050) | Val Loss=0.121759 \n",
      "Epoch 048 | Train Loss=0.115090 (box=0.0101, obj=0.0093, cls=0.0050) | Val Loss=0.132578 \n",
      "Epoch 049 | Train Loss=0.113741 (box=0.0100, obj=0.0093, cls=0.0050) | Val Loss=0.104739 \n",
      " ✅ Saved new best model (val_loss=0.1047)\n",
      "Epoch 050 | Train Loss=0.113040 (box=0.0099, obj=0.0093, cls=0.0050) | Val Loss=0.104505 \n",
      " ✅ Saved new best model (val_loss=0.1045)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "save_dir = \"./checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model = Path1Model(num_classes=2).to(device)\n",
    "\n",
    "# ✅ Freeze backbone if needed\n",
    "for p in model.backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "criterion = DetectionLoss(num_classes=2).to(device)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "train_box_loss = 0.0\n",
    "train_obj_loss = 0.0\n",
    "train_cls_loss = 0.0\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, targets in train_loader:   # targets is a dict from collate_fn\n",
    "        images = images.to(device)\n",
    "\n",
    "        # ✅ Split back into list of dicts for each image\n",
    "        batch_targets = []\n",
    "        start = 0\n",
    "        \n",
    "        for count in targets[\"gt_groups\"]:\n",
    "            count = count.item()\n",
    "            boxes = targets[\"bboxes\"][start:start+count].to(device)\n",
    "            labels = targets[\"cls\"][start:start+count].to(device)\n",
    "            start += count\n",
    "\n",
    "            batch_targets.append({\"boxes\": boxes, \"labels\": labels})\n",
    "\n",
    "        # -------------------- Forward pass --------------------\n",
    "        preds = model({\"pixel_values\": images})  # list of detection heads\n",
    "\n",
    "        # -------------------- Compute loss --------------------\n",
    "        loss, loss_items = criterion(preds, batch_targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_box_loss += loss_items[\"box_loss\"]\n",
    "        train_obj_loss += loss_items[\"obj_loss\"]\n",
    "        train_cls_loss += loss_items[\"cls_loss\"]\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_box_loss /= len(train_loader)\n",
    "    train_obj_loss /= len(train_loader)\n",
    "    train_cls_loss /= len(train_loader)\n",
    "\n",
    "    # -------------------- Validation --------------------\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            batch_targets = []\n",
    "            start = 0\n",
    "            for count in targets[\"gt_groups\"]:\n",
    "                count = count.item()\n",
    "                boxes = targets[\"bboxes\"][start:start+count].to(device)\n",
    "                labels = targets[\"cls\"][start:start+count].to(device)\n",
    "                start += count\n",
    "                batch_targets.append({\"boxes\": boxes, \"labels\": labels})\n",
    "\n",
    "\n",
    "            preds = model({\"pixel_values\": images})\n",
    "            loss, _ = criterion(preds, batch_targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    # -------------------- Logging --------------------\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:03d} | \"\n",
    "        f\"Train Loss={train_loss:.6f} (box={train_box_loss:.4f}, obj={train_obj_loss:.4f}, cls={train_cls_loss:.4f}) | \"\n",
    "        f\"Val Loss={val_loss:.6f} \"\n",
    "    )\n",
    "\n",
    "    # -------------------- Save best checkpoint --------------------\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, \"best_model.pth\"))\n",
    "        print(f\" ✅ Saved new best model (val_loss={val_loss:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T22:32:10.916464Z",
     "iopub.status.busy": "2025-09-11T22:32:10.916166Z",
     "iopub.status.idle": "2025-09-11T22:32:10.925648Z",
     "shell.execute_reply": "2025-09-11T22:32:10.924987Z",
     "shell.execute_reply.started": "2025-09-11T22:32:10.916443Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path1Model(\n",
       "  (backbone): DINOBackbone(\n",
       "    (model): DINOv3ConvNextModel(\n",
       "      (stages): ModuleList(\n",
       "        (0): DINOv3ConvNextStage(\n",
       "          (downsample_layers): ModuleList(\n",
       "            (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "            (1): DINOv3ConvNextLayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (layers): ModuleList(\n",
       "            (0-2): 3 x DINOv3ConvNextLayer(\n",
       "              (depthwise_conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
       "              (layer_norm): DINOv3ConvNextLayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "              (pointwise_conv1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (activation_fn): GELUActivation()\n",
       "              (pointwise_conv2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): DINOv3ConvNextStage(\n",
       "          (downsample_layers): ModuleList(\n",
       "            (0): DINOv3ConvNextLayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "            (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "          )\n",
       "          (layers): ModuleList(\n",
       "            (0-2): 3 x DINOv3ConvNextLayer(\n",
       "              (depthwise_conv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "              (layer_norm): DINOv3ConvNextLayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "              (pointwise_conv1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (activation_fn): GELUActivation()\n",
       "              (pointwise_conv2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): DINOv3ConvNextStage(\n",
       "          (downsample_layers): ModuleList(\n",
       "            (0): DINOv3ConvNextLayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "            (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "          )\n",
       "          (layers): ModuleList(\n",
       "            (0-26): 27 x DINOv3ConvNextLayer(\n",
       "              (depthwise_conv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "              (layer_norm): DINOv3ConvNextLayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "              (pointwise_conv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (activation_fn): GELUActivation()\n",
       "              (pointwise_conv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): DINOv3ConvNextStage(\n",
       "          (downsample_layers): ModuleList(\n",
       "            (0): DINOv3ConvNextLayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "            (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
       "          )\n",
       "          (layers): ModuleList(\n",
       "            (0-2): 3 x DINOv3ConvNextLayer(\n",
       "              (depthwise_conv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
       "              (layer_norm): DINOv3ConvNextLayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "              (pointwise_conv1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (activation_fn): GELUActivation()\n",
       "              (pointwise_conv2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop_path): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (pool): AdaptiveAvgPool2d(output_size=1)\n",
       "    )\n",
       "  )\n",
       "  (head): YoloDetectionHeadPath1(\n",
       "    (reduce_p3): Conv(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (reduce_p4): Conv(\n",
       "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (reduce_p5): Conv(\n",
       "      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (up_p5): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (c3_p4): C3k2(\n",
       "      (conv1): Conv(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): SiLU()\n",
       "      )\n",
       "      (conv2): Conv(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): SiLU()\n",
       "      )\n",
       "      (res_blocks): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv(\n",
       "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): SiLU()\n",
       "          )\n",
       "          (conv2): Conv(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv3): Conv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (up_p4): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (c3_p3): C3k2(\n",
       "      (conv1): Conv(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): SiLU()\n",
       "      )\n",
       "      (conv2): Conv(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): SiLU()\n",
       "      )\n",
       "      (res_blocks): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv(\n",
       "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): SiLU()\n",
       "          )\n",
       "          (conv2): Conv(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv3): Conv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (down_p3): Conv(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (c3_p4d): C3k2(\n",
       "      (conv1): Conv(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): SiLU()\n",
       "      )\n",
       "      (conv2): Conv(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): SiLU()\n",
       "      )\n",
       "      (res_blocks): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv(\n",
       "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): SiLU()\n",
       "          )\n",
       "          (conv2): Conv(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv3): Conv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (down_p4): Conv(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (c3_p5d): C3k2(\n",
       "      (conv1): Conv(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): SiLU()\n",
       "      )\n",
       "      (conv2): Conv(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): SiLU()\n",
       "      )\n",
       "      (res_blocks): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv(\n",
       "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): SiLU()\n",
       "          )\n",
       "          (conv2): Conv(\n",
       "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv3): Conv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (detect): Detect(\n",
       "      (cv2): ModuleList(\n",
       "        (0-2): 3 x Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (cv3): ModuleList(\n",
       "        (0-2): 3 x Conv2d(256, 7, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T22:42:40.435038Z",
     "iopub.status.busy": "2025-09-11T22:42:40.434269Z",
     "iopub.status.idle": "2025-09-11T22:42:40.441804Z",
     "shell.execute_reply": "2025-09-11T22:42:40.441167Z",
     "shell.execute_reply.started": "2025-09-11T22:42:40.435013Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_dataset  = FireDataset(\"/kaggle/input/home-fire-dataset/test\", processor)\n",
    "\n",
    "test_loader = DataLoader(  \n",
    "    test_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T22:48:15.733324Z",
     "iopub.status.busy": "2025-09-11T22:48:15.733033Z",
     "iopub.status.idle": "2025-09-11T22:48:15.737463Z",
     "shell.execute_reply": "2025-09-11T22:48:15.736677Z",
     "shell.execute_reply.started": "2025-09-11T22:48:15.733303Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_fn_raw(batch):\n",
    "    images, targets = zip(*batch)   # keep as lists\n",
    "    return list(images), list(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T22:34:37.428634Z",
     "iopub.status.busy": "2025-09-11T22:34:37.428315Z",
     "iopub.status.idle": "2025-09-11T22:35:54.262804Z",
     "shell.execute_reply": "2025-09-11T22:35:54.262086Z",
     "shell.execute_reply.started": "2025-09-11T22:34:37.428613Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Avg loss=0.1115 (box=0.0097, obj=0.0094, cls=0.0052)\n"
     ]
    }
   ],
   "source": [
    "# -------------------- TEST --------------------\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_box_loss = 0.0\n",
    "test_obj_loss = 0.0\n",
    "test_cls_loss = 0.0\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        # ✅ Same target splitting logic as training/val\n",
    "        batch_targets = []\n",
    "        start = 0\n",
    "        for count in targets[\"gt_groups\"]:\n",
    "            count = count.item()\n",
    "            boxes = targets[\"bboxes\"][start:start+count].to(device)\n",
    "            labels = targets[\"cls\"][start:start+count].to(device)\n",
    "            start += count\n",
    "            batch_targets.append({\"boxes\": boxes, \"labels\": labels})\n",
    "\n",
    "        # Forward + loss\n",
    "        preds = model({\"pixel_values\": images})\n",
    "        loss, loss_items = criterion(preds, batch_targets)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        test_box_loss += loss_items[\"box_loss\"]\n",
    "        test_obj_loss += loss_items[\"obj_loss\"]\n",
    "        test_cls_loss += loss_items[\"cls_loss\"]\n",
    "\n",
    "        # save preds + targets for metrics\n",
    "        all_preds.extend(preds)\n",
    "        all_targets.extend(batch_targets)\n",
    "\n",
    "# ✅ Average over test set\n",
    "test_loss /= len(test_loader)\n",
    "test_box_loss /= len(test_loader)\n",
    "test_obj_loss /= len(test_loader)\n",
    "test_cls_loss /= len(test_loader)\n",
    "\n",
    "print(\n",
    "    f\"[TEST] Avg loss={test_loss:.4f} \"\n",
    "    f\"(box={test_box_loss:.4f}, obj={test_obj_loss:.4f}, cls={test_cls_loss:.4f})\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7370967,
     "sourceId": 11741902,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
